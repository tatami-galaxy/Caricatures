{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Abstraction Analysis with Distributed Alignment Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Atticus Geiger\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Contents\n",
    "\n",
    "1. [The hierarchical equality task](#The-hierarchical-equality-task)\n",
    "    1. [An Algorithm that Solves the Equality Task](#An-Algorithm-that-Solves-the-Equality-Task)\n",
    "        1. [The algorithm with no intervention](#The-algorithm-with-no-intervention)\n",
    "        1. [The algorithm with an intervention](#The-algorithm-with-an-intervention)\n",
    "        1. [The algorithm with an interchange intervention](#The-algorithm-with-an-interchange-intervention)\n",
    "    1. [Hand Crafting an MLP to Solve Hierarchical Equality](#Hand-Crafting-an-MLP-to-Solve-Hierarchical-Equality)        \n",
    "    1. [Training an MLP to Solve Hierarchical Equality](#Training-an-MLP-to-Solve-Hierarchical-Equality)\n",
    "1. [Causal abstraction Analysis](#Causal-abstraction)\n",
    "    1. [Basic intervention: zeroing out part of a hidden layer](#Basic-intervention:-zeroing-out-part-of-a-hidden-layer)\n",
    "    1. [An interchange intervention](#An-interchange-intervention)\n",
    "    1. [Alignment](#Alignment)\n",
    "    1. [Evaluating an Alignment](#Evaluation)\n",
    "1. [Distributed Alignment Search (DAS)](#Distributed-Alignment-Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "\n",
    "This notebook is a hands-on introduction to __causal abstraction analysis__ [Geiger*, Lu*, Icard, and Potts (2020)](https://arxiv.org/pdf/2106.02997.pdf) using __distributed alignment search__ [Geiger*, Wu*, Potts, Icard, and Goodman (2020)](https://arxiv.org/pdf/2303.02536.pdf).\n",
    "\n",
    "In causal abstraction analysis, we assess whether trained models conform to high-level causal models that we specify, not just in terms of their inputâ€“output behavior, but also in terms of their internal dynamics. The core technique is the __interchange intervention__, in which a causal model is provided an input and then intermediate variables are fixed to take on the values they would have for a second input.\n",
    "\n",
    "To motivate and illustrate these concepts, we're going to focus on a hierarchical equality task, building on work by [Geiger, Carstensen, Frank, and Potts (2020)](https://arxiv.org/abs/2006.07968)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "import random\n",
    "import copy\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from pyvene.models.mlp.modelings_mlp import MLPConfig\n",
    "from pyvene import create_mlp_classifier\n",
    "from pyvene import (\n",
    "    VanillaIntervention,\n",
    "    RotatedSpaceIntervention,\n",
    "    LowRankRotatedSpaceIntervention,\n",
    "    RepresentationConfig,\n",
    "    IntervenableConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x112a63e90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intervenable Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, logging, torch, types\n",
    "import nnsight\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from typing import List, Optional, Tuple, Union, Dict, Any\n",
    "\n",
    "from pyvene.models.constants import *\n",
    "from pyvene.models.basic_utils import *\n",
    "from pyvene.models.modeling_utils import *\n",
    "from pyvene.models.intervention_utils import *\n",
    "from pyvene.models.interventions import *\n",
    "from pyvene.models.configuration_intervenable_model import (\n",
    "    IntervenableConfig,\n",
    "    RepresentationConfig,\n",
    ")\n",
    "from pyvene.models.interventions import (\n",
    "    TrainableIntervention,\n",
    "    SkipIntervention,\n",
    "    CollectIntervention,\n",
    "    BoundlessRotatedSpaceIntervention\n",
    ")\n",
    "\n",
    "from torch import optim\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from dataclasses import dataclass\n",
    "from transformers.utils import ModelOutput\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "@dataclass\n",
    "class IntervenableModelOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Output of the IntervenableModel, including original outputs, intervened outputs, and collected activations.\n",
    "    \"\"\"\n",
    "    original_outputs: Optional[Any] = None\n",
    "    intervened_outputs: Optional[Any] = None\n",
    "    collected_activations: Optional[Any] = None\n",
    "\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Base model class for sharing static vars and methods.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, model, backend, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        super().__init__()\n",
    "        if isinstance(config, dict) or isinstance(config, list):\n",
    "            config = IntervenableConfig(\n",
    "                representations = config\n",
    "            )\n",
    "        self.config = config\n",
    "        \n",
    "        self.mode = config.mode\n",
    "        intervention_type = config.intervention_types\n",
    "        self.is_model_stateless = is_stateless(model)\n",
    "        self.config.model_type = str(type(model)) # backfill\n",
    "        self.use_fast = kwargs[\"use_fast\"] if \"use_fast\" in kwargs else False\n",
    "\n",
    "        self.model_has_grad = False\n",
    "        if self.use_fast:\n",
    "            logging.warn(\n",
    "                \"Detected use_fast=True means the intervention location \"\n",
    "                \"will be static within a batch.\\n\\nIn case multiple \"\n",
    "                \"location tags are passed only the first one will \"\n",
    "                \"be considered\"\n",
    "            )\n",
    "        # each representation can get a different intervention type\n",
    "        if type(intervention_type) == list:\n",
    "            assert len(intervention_type) == len(\n",
    "                config.representations\n",
    "            )\n",
    "\n",
    "        ###\n",
    "        # We instantiate intervention_layers at locations.\n",
    "        # Note that the layer name mentioned in the config is\n",
    "        # abstract. Not the actual module name of the model.\n",
    "        #\n",
    "        # This script will automatically convert abstract\n",
    "        # name into module name if the model type is supported.\n",
    "        #\n",
    "        # To support a new model type, you need to provide a\n",
    "        # mapping between supported abstract type and module name.\n",
    "        ###\n",
    "        self.representations = {}\n",
    "        self.interventions = {}\n",
    "        self._key_collision_counter = {}\n",
    "        self.return_collect_activations = False\n",
    "        # Flags and counters below are for interventions in the model.generate\n",
    "        # call. We can intervene on the prompt tokens only, on each generated\n",
    "        # token, or on a combination of both.\n",
    "        self._is_generation = False\n",
    "        self._intervene_on_prompt = None\n",
    "        self._key_getter_call_counter = {}\n",
    "        self._key_setter_call_counter = {}\n",
    "        self._intervention_pointers = {}\n",
    "        self._intervention_reverse_link = {}\n",
    "\n",
    "        # hooks are stateful internally, meaning that it's aware of how many times\n",
    "        # it is called during the execution.\n",
    "        # TODO: this could be merged with call counter above later.\n",
    "        self._intervention_state = {}\n",
    "\n",
    "        # We want to associate interventions with a group to do group-wise interventions.\n",
    "        self._intervention_group = {}\n",
    "        _any_group_key = False\n",
    "        _original_key_order = []\n",
    "        for i, representation in enumerate(\n",
    "            config.representations\n",
    "        ):\n",
    "            _key = self._get_representation_key(representation)\n",
    "\n",
    "            # get intervention type\n",
    "            if representation.intervention is not None:\n",
    "                intervention = representation.intervention\n",
    "                intervention.use_fast = self.use_fast\n",
    "            else:\n",
    "                intervention_function = (\n",
    "                    intervention_type\n",
    "                    if type(intervention_type) != list\n",
    "                    else intervention_type[i]\n",
    "                )\n",
    "                all_metadata = representation._asdict()\n",
    "                component_dim = get_dimension_by_component(\n",
    "                    get_internal_model_type(model), model.config, \n",
    "                    representation.component\n",
    "                )\n",
    "                if component_dim is not None:\n",
    "                    component_dim *= int(representation.max_number_of_units)\n",
    "                all_metadata[\"embed_dim\"] = component_dim\n",
    "                all_metadata[\"use_fast\"] = self.use_fast\n",
    "                # initialize intervention class\n",
    "                intervention = intervention_function(**all_metadata)\n",
    "                \n",
    "            if representation.intervention_link_key in self._intervention_pointers:\n",
    "                self._intervention_reverse_link[\n",
    "                    _key\n",
    "                ] = f\"link#{representation.intervention_link_key}\"\n",
    "                intervention = self._intervention_pointers[\n",
    "                    representation.intervention_link_key\n",
    "                ]\n",
    "            elif representation.intervention_link_key is not None:\n",
    "                self._intervention_pointers[\n",
    "                    representation.intervention_link_key\n",
    "                ] = intervention\n",
    "                self._intervention_reverse_link[\n",
    "                    _key\n",
    "                ] = f\"link#{representation.intervention_link_key}\"\n",
    "                    \n",
    "            if isinstance(\n",
    "                intervention,\n",
    "                CollectIntervention\n",
    "            ):\n",
    "                self.return_collect_activations = True\n",
    "\n",
    "            # determine what kind of hooks are needed\n",
    "            # determine model component to intervene with hook\n",
    "            # get_module_hook in modeling_utils.py\n",
    "            # hook is determined by model component\n",
    "            module_hook = get_module_hook(\n",
    "                model, representation, backend\n",
    "            )\n",
    "            self.representations[_key] = representation\n",
    "            self.interventions[_key] = (intervention, module_hook)\n",
    "            self._key_getter_call_counter[\n",
    "                _key\n",
    "            ] = 0  # we memo how many the hook is called,\n",
    "            # usually, it's a one time call per\n",
    "            # hook unless model generates.\n",
    "            self._key_setter_call_counter[_key] = 0\n",
    "            self._intervention_state[_key] = InterventionState(_key)\n",
    "            _original_key_order += [_key]\n",
    "            if representation.group_key is not None:\n",
    "                _any_group_key = True\n",
    "        if self.config.sorted_keys is not None:\n",
    "            logging.warn(\n",
    "                \"The key is provided in the config. \"\n",
    "                \"Assuming this is loaded from a pretrained module.\"\n",
    "            )\n",
    "        if (\n",
    "            self.config.sorted_keys is not None\n",
    "            or \"intervenables_sort_fn\" not in kwargs\n",
    "        ):\n",
    "            self.sorted_keys = _original_key_order\n",
    "        else:\n",
    "            # the key order is independent of group, it is used to read out intervention locations.\n",
    "            self.sorted_keys = kwargs[\"intervenables_sort_fn\"](\n",
    "                model, self.representations\n",
    "            )\n",
    "\n",
    "        \"\"\"\n",
    "        We later use _intervention_group to run actual interventions.\n",
    "        The order the group by group; and there should not be dependency\n",
    "        between groups.\n",
    "        \"\"\"\n",
    "        \n",
    "        if _any_group_key:\n",
    "            # In case they are grouped, we would expect the execution order is given\n",
    "            # by the source inputs.\n",
    "            _validate_group_keys = []\n",
    "            for _key in self.sorted_keys:\n",
    "                representation = self.representations[_key]\n",
    "                assert representation.group_key is not None\n",
    "                if representation.group_key in self._intervention_group:\n",
    "                    self._intervention_group[representation.group_key].append(_key)\n",
    "                else:\n",
    "                    self._intervention_group[representation.group_key] = [_key]\n",
    "                _validate_group_keys += [representation.group_key]\n",
    "            for i in range(len(_validate_group_keys) - 1):\n",
    "                if _validate_group_keys[i] > _validate_group_keys[i + 1]:\n",
    "                    logging.info(\n",
    "                        f\"This is not a valid group key order: {_validate_group_keys}\" \n",
    "                    )\n",
    "                    raise ValueError(\n",
    "                        \"Must be ascending order. \"\n",
    "                        \"Interventions would be performed in order within group as well\"\n",
    "                    )\n",
    "        else:\n",
    "            # assign each key to an unique group based on topological order\n",
    "            _group_key_inc = 0\n",
    "            for _key in self.sorted_keys:\n",
    "                self._intervention_group[_group_key_inc] = [_key]\n",
    "                _group_key_inc += 1\n",
    "        # sort group key with ascending order\n",
    "        self._intervention_group = OrderedDict(sorted(self._intervention_group.items()))\n",
    "\n",
    "        # cached swap-in activations\n",
    "        self.activations = {}\n",
    "        # cached swapped activations (hot)\n",
    "        self.hot_activations = {}\n",
    "\n",
    "        # temp fields should not be accessed outside\n",
    "        self._batched_setter_activation_select = {}\n",
    "        \"\"\"\n",
    "        Activations in the future list is ALWAYS causally before\n",
    "        the vanilla activation list. This field becomes crucial\n",
    "        if we intervene at the same place multiple times.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.model_config = model.config\n",
    "        self.model_type = get_internal_model_type(model)\n",
    "        self.disable_model_gradients()\n",
    "        self.trainable_model_parameters = {}\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Print out basic info about this intervenable instance\n",
    "        Not working!\n",
    "        \"\"\"\n",
    "        attr_dict = {\n",
    "            \"model_type\": self.model_type,\n",
    "            #\"intervention_types\": self.config.intervention_types.__name__,\n",
    "            \"alignabls\": self.sorted_keys,\n",
    "            \"mode\": self.mode,\n",
    "            \"config\": self.config,\n",
    "            \"representations\": self.representations,\n",
    "            \"interventions\": self.interventions,\n",
    "        }\n",
    "        return json.dumps(attr_dict, indent=4)\n",
    "        #return attr_dict\n",
    "\n",
    "\n",
    "    def _get_representation_key(self, representation):\n",
    "        \"\"\"\n",
    "        Provide unique key for each intervention\n",
    "        \"\"\"\n",
    "        l = representation.layer\n",
    "        c = representation.component\n",
    "        u = representation.unit\n",
    "        n = representation.max_number_of_units\n",
    "        if \".\" in c:\n",
    "            # string access for sure\n",
    "            key_proposal = f\"comp.{c}.unit.{u}.nunit.{n}\"\n",
    "        else:\n",
    "            key_proposal = f\"layer.{l}.comp.{c}.unit.{u}.nunit.{n}\"\n",
    "        if key_proposal not in self._key_collision_counter:\n",
    "            self._key_collision_counter[key_proposal] = 0\n",
    "        else:\n",
    "            self._key_collision_counter[key_proposal] += 1\n",
    "        return f\"{key_proposal}#{self._key_collision_counter[key_proposal]}\"\n",
    "    \n",
    "\n",
    "    def get_trainable_parameters(self):\n",
    "        \"\"\"\n",
    "        Return trainable params as key value pairs\n",
    "        \"\"\"\n",
    "        ret_params = []\n",
    "        for k, v in self.interventions.items():\n",
    "            if isinstance(v[0], TrainableIntervention):\n",
    "                ret_params += [p for p in v[0].parameters()]\n",
    "        for p in self.model.parameters():\n",
    "            if p.requires_grad:\n",
    "                ret_params += [p]\n",
    "        return ret_params\n",
    "    \n",
    "\n",
    "    def named_parameters(self, recurse=True):\n",
    "        \"\"\"\n",
    "        The above, but for HuggingFace.\n",
    "        \"\"\"\n",
    "        ret_params = []\n",
    "        for k, v in self.interventions.items():\n",
    "            if isinstance(v[0], TrainableIntervention):\n",
    "                ret_params += [(k + '.' + n, p) for n, p in v[0].named_parameters()]\n",
    "        for n, p in self.model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                ret_params += [('model.' + n, p)]\n",
    "        return ret_params\n",
    "    \n",
    "\n",
    "    def get_cached_activations(self):\n",
    "        \"\"\"\n",
    "        Return the cached activations with keys\n",
    "        \"\"\"\n",
    "        return self.activations\n",
    "\n",
    "\n",
    "    def get_cached_hot_activations(self):\n",
    "        \"\"\"\n",
    "        Return the cached hot activations with linked keys\n",
    "        \"\"\"\n",
    "        return self.hot_activations\n",
    "\n",
    "\n",
    "    def set_temperature(self, temp: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Set temperature if needed\n",
    "        \"\"\"\n",
    "        for k, v in self.interventions.items():\n",
    "            if isinstance(v[0], BoundlessRotatedSpaceIntervention) or \\\n",
    "                isinstance(v[0], SigmoidMaskIntervention):\n",
    "                v[0].set_temperature(temp)\n",
    "\n",
    "\n",
    "    def enable_model_gradients(self):\n",
    "        \"\"\"\n",
    "        Enable gradient in the model\n",
    "        \"\"\"\n",
    "        # Unfreeze all model weights\n",
    "        self.model.train()\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True \n",
    "        self.model_has_grad = True\n",
    "\n",
    "\n",
    "    def disable_model_gradients(self):\n",
    "        \"\"\"\n",
    "        Disable gradient in the model\n",
    "        \"\"\"\n",
    "        # Freeze all model weights\n",
    "        self.model.eval()\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.model_has_grad = False\n",
    "\n",
    "\n",
    "    def disable_intervention_gradients(self):\n",
    "        \"\"\"\n",
    "        Disable gradient in the trainable intervention\n",
    "        \"\"\"\n",
    "        # Freeze all intervention weights\n",
    "        pass\n",
    "\n",
    "\n",
    "    def set_device(self, device, set_model=True):\n",
    "        \"\"\"\n",
    "        Set device of interventions and the model\n",
    "        \"\"\"\n",
    "        for k, v in self.interventions.items():\n",
    "            v[0].to(device)\n",
    "        if set_model:\n",
    "            self.model.to(device)\n",
    "\n",
    "\n",
    "    def get_device(self):\n",
    "        \"\"\"\n",
    "        Get device of interventions and the model\n",
    "        \"\"\"\n",
    "        return self.model.device\n",
    "\n",
    "\n",
    "    def count_parameters(self, include_model=False):\n",
    "        \"\"\"\n",
    "        Set device of interventions and the model\n",
    "        \"\"\"\n",
    "        _linked_key_set = set([])\n",
    "        total_parameters = 0\n",
    "        for k, v in self.interventions.items():\n",
    "            if isinstance(v[0], TrainableIntervention):\n",
    "                if k in self._intervention_reverse_link:\n",
    "                    if not self._intervention_reverse_link[k] in _linked_key_set:\n",
    "                        _linked_key_set.add(self._intervention_reverse_link[k])\n",
    "                        total_parameters += count_parameters(v[0])\n",
    "                else:\n",
    "                    total_parameters += count_parameters(v[0])\n",
    "        if include_model:\n",
    "            total_parameters += sum(\n",
    "                p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        return total_parameters\n",
    "\n",
    "\n",
    "    def set_zero_grad(self):\n",
    "        \"\"\"\n",
    "        Set device of interventions and the model\n",
    "        \"\"\"\n",
    "        for k, v in self.interventions.items():\n",
    "            if isinstance(v[0], TrainableIntervention):\n",
    "                v[0].zero_grad()\n",
    "\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        The above, but for HuggingFace.\n",
    "        \"\"\"\n",
    "        for k, v in self.interventions.items():\n",
    "            if isinstance(v[0], TrainableIntervention):\n",
    "                v[0].zero_grad()\n",
    "\n",
    "\n",
    "    def _input_validation(\n",
    "        self,\n",
    "        base,\n",
    "        sources,\n",
    "        unit_locations,\n",
    "        activations_sources,\n",
    "        subspaces,\n",
    "    ):\n",
    "        \"\"\"Fail fast input validation\"\"\"\n",
    "        if self.mode == \"parallel\" and unit_locations is not None:\n",
    "            assert \"sources->base\" in unit_locations or \"base\" in unit_locations\n",
    "        elif activations_sources is None and unit_locations is not None and self.mode == \"serial\":\n",
    "            assert \"sources->base\" not in unit_locations\n",
    "        \n",
    "        # sources may contain None, but length should match\n",
    "        if sources is not None and not (len(sources) == 1 and sources[0] == None):\n",
    "            if len(sources) != len(self._intervention_group):\n",
    "                raise ValueError(\n",
    "                    f\"Source length {len(sources)} is not \"\n",
    "                    f\"equal to intervention length {len(self._intervention_group)}.\"\n",
    "                )\n",
    "        elif activations_sources is not None:\n",
    "            if len(activations_sources) != len(self._intervention_group):\n",
    "                raise ValueError(\n",
    "                    f\"Source activations length {len(activations_sources)} is not \"\n",
    "                    f\"equal to intervention length {len(self._intervention_group)}.\"\n",
    "                )\n",
    "\n",
    "        # if it is stateful models, the passed in activations need to have states\n",
    "        if not self.is_model_stateless and activations_sources is not None:\n",
    "            for _, v in activations_sources.items():\n",
    "                if (\n",
    "                    isinstance(v, list)\n",
    "                    and isinstance(v[0], tuple)\n",
    "                    and isinstance(v[0][1], list) != True\n",
    "                ):\n",
    "                    raise ValueError(\n",
    "                        f\"Stateful models need nested activations. See our documentions.\"\n",
    "                    )\n",
    "\n",
    "\n",
    "    def _gather_intervention_output(\n",
    "        self, output, representations_key, unit_locations\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Gather intervening activations from the output based on indices\n",
    "        \"\"\"\n",
    "        if (\n",
    "            representations_key in self._intervention_reverse_link\n",
    "            and self._intervention_reverse_link[representations_key]\n",
    "            in self.hot_activations\n",
    "        ):\n",
    "            # hot gather\n",
    "            # clone is needed here by acting as a different module\n",
    "            # to avoid gradient conflict.\n",
    "            #\n",
    "            # enable the following line when an error is hit\n",
    "            # torch.autograd.set_detect_anomaly(True)\n",
    "            selected_output = self.hot_activations[\n",
    "                self._intervention_reverse_link[representations_key]\n",
    "            ]\n",
    "        else:\n",
    "            # data structure casting\n",
    "            if isinstance(output, tuple):\n",
    "                original_output = output[0].clone()\n",
    "            else:\n",
    "                original_output = output.clone()\n",
    "            # for non-sequence models, there is no concept of\n",
    "            # unit location anyway.\n",
    "            if unit_locations is None:\n",
    "                return original_output\n",
    "            # gather subcomponent\n",
    "            original_output = output_to_subcomponent(\n",
    "                original_output,\n",
    "                self.representations[\n",
    "                    representations_key\n",
    "                ].component,\n",
    "                self.model_type,\n",
    "                self.model_config,\n",
    "            )\n",
    "\n",
    "            # gather based on intervention locations\n",
    "            selected_output = gather_neurons(\n",
    "                original_output,\n",
    "                self.representations[\n",
    "                    representations_key\n",
    "                ].unit,\n",
    "                unit_locations,\n",
    "                device=self.get_device()\n",
    "            )\n",
    "\n",
    "        return selected_output\n",
    "\n",
    "\n",
    "    def _scatter_intervention_output(\n",
    "        self,\n",
    "        output,\n",
    "        intervened_representation,\n",
    "        representations_key,\n",
    "        unit_locations,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Scatter in the intervened activations in the output\n",
    "        \"\"\"\n",
    "        # data structure casting\n",
    "        if isinstance(output, tuple):\n",
    "            original_output = output[0]\n",
    "        else:\n",
    "            original_output = output\n",
    "        # for non-sequence-based models, we simply replace\n",
    "        # all the activations.\n",
    "        if unit_locations is None:\n",
    "            original_output[:] = intervened_representation[:]\n",
    "            return original_output\n",
    "        \n",
    "        # ex : block_input\n",
    "        component = self.representations[\n",
    "            representations_key\n",
    "        ].component\n",
    "        unit = self.representations[\n",
    "            representations_key\n",
    "        ].unit\n",
    "     \n",
    "        # scatter in-place\n",
    "        _ = scatter_neurons(\n",
    "            original_output,\n",
    "            intervened_representation,\n",
    "            component,\n",
    "            unit,\n",
    "            unit_locations,\n",
    "            self.model_type,\n",
    "            self.model_config,\n",
    "            self.use_fast,\n",
    "            device=self.get_device()\n",
    "        )\n",
    "        \n",
    "        return original_output\n",
    "    \n",
    "\n",
    "    def _broadcast_unit_locations(\n",
    "        self,\n",
    "        batch_size,\n",
    "        unit_locations\n",
    "    ):\n",
    "        if unit_locations is None:\n",
    "            # this means, we don't filter based on location at all.\n",
    "            return {\"sources->base\": ([None]*len(self.interventions), [None]*len(self.interventions))}\n",
    "        \n",
    "        if self.mode == \"parallel\":\n",
    "            _unit_locations = {}\n",
    "            for k, v in unit_locations.items():\n",
    "                # special broadcast for base-only interventions\n",
    "                is_base_only = False\n",
    "                if k == \"base\":\n",
    "                    is_base_only = True\n",
    "                    k = \"sources->base\"\n",
    "                if isinstance(v, int):\n",
    "                    if is_base_only:\n",
    "                        _unit_locations[k] = (None, [[[v]]*batch_size]*len(self.interventions))\n",
    "                    else:\n",
    "                        _unit_locations[k] = (\n",
    "                            [[[v]]*batch_size]*len(self.interventions), \n",
    "                            [[[v]]*batch_size]*len(self.interventions)\n",
    "                        )\n",
    "                    self.use_fast = True\n",
    "                elif len(v) == 2 and isinstance(v[0], int) and isinstance(v[1], int):\n",
    "                    _unit_locations[k] = (\n",
    "                        [[[v[0]]]*batch_size]*len(self.interventions), \n",
    "                        [[[v[1]]]*batch_size]*len(self.interventions)\n",
    "                    )\n",
    "                    self.use_fast = True\n",
    "                elif len(v) == 2 and v[0] == None and isinstance(v[1], int):\n",
    "                    _unit_locations[k] = (None, [[[v[1]]]*batch_size]*len(self.interventions))\n",
    "                    self.use_fast = True\n",
    "                elif len(v) == 2 and isinstance(v[0], int) and v[1] == None:\n",
    "                    _unit_locations[k] = ([[[v[0]]]*batch_size]*len(self.interventions), None)\n",
    "                    self.use_fast = True\n",
    "                elif isinstance(v, list) and get_list_depth(v) == 1:\n",
    "                    # [0,1,2,3] -> [[[0,1,2,3]]], ...\n",
    "                    if is_base_only:\n",
    "                        _unit_locations[k] = (None, [[v]*batch_size]*len(self.interventions))\n",
    "                    else:\n",
    "                        _unit_locations[k] = (\n",
    "                            [[v]*batch_size]*len(self.interventions), \n",
    "                            [[v]*batch_size]*len(self.interventions)\n",
    "                        )\n",
    "                    self.use_fast = True\n",
    "                else:\n",
    "                    if is_base_only:\n",
    "                        _unit_locations[k] = (None, v)\n",
    "                    else:\n",
    "                        _unit_locations[k] = v\n",
    "        elif self.mode == \"serial\":\n",
    "            _unit_locations = {}\n",
    "            for k, v in unit_locations.items():\n",
    "                if isinstance(v, int):\n",
    "                    _unit_locations[k] = (\n",
    "                        [[[v]]*batch_size]*len(self.interventions), \n",
    "                        [[[v]]*batch_size]*len(self.interventions)\n",
    "                    )\n",
    "                    self.use_fast = True\n",
    "                elif len(v) == 2 and isinstance(v[0], int) and isinstance(v[1], int):\n",
    "                    _unit_locations[k] = (\n",
    "                        [[[v[0]]]*batch_size]*len(self.interventions), \n",
    "                        [[[v[1]]]*batch_size]*len(self.interventions)\n",
    "                    )\n",
    "                    self.use_fast = True\n",
    "                elif len(v) == 2 and v[0] == None and isinstance(v[1], int):\n",
    "                    _unit_locations[k] = (None, [[[v[1]]]*batch_size]*len(self.interventions))\n",
    "                    self.use_fast = True\n",
    "                elif len(v) == 2 and isinstance(v[0], int) and v[1] == None:\n",
    "                    _unit_locations[k] = ([[[v[0]]]*batch_size]*len(self.interventions), None)\n",
    "                    self.use_fast = True\n",
    "                elif isinstance(v, list) and get_list_depth(v) == 1:\n",
    "                    # [0,1,2,3] -> [[[0,1,2,3]]], ...\n",
    "                    _unit_locations[k] = (\n",
    "                        [[v]*batch_size]*len(self.interventions), \n",
    "                        [[v]*batch_size]*len(self.interventions)\n",
    "                    )\n",
    "                    self.use_fast = True\n",
    "                else:\n",
    "                    _unit_locations[k] = v\n",
    "        else:\n",
    "            raise ValueError(f\"The mode {self.mode} is not supported.\")\n",
    "        return _unit_locations\n",
    "    \n",
    "\n",
    "    def _broadcast_source_representations(\n",
    "        self,\n",
    "        source_representations\n",
    "    ):\n",
    "        \"\"\"Broadcast simple inputs to a dict\"\"\"\n",
    "        _source_representations = {}\n",
    "        if isinstance(source_representations, dict) or source_representations is None:\n",
    "            # pass to broadcast for advance usage\n",
    "            _source_representations = source_representations\n",
    "        elif isinstance(source_representations, list):\n",
    "            for i, key in enumerate(self.sorted_keys):\n",
    "                _source_representations[key] = source_representations[i]\n",
    "        elif isinstance(source_representations, torch.Tensor):\n",
    "            for key in self.sorted_keys:\n",
    "                _source_representations[key] = source_representations\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Accept input type for source_representations is [Dict, List, torch.Tensor]\"\n",
    "            )\n",
    "        return _source_representations\n",
    "            \n",
    "\n",
    "    def _broadcast_sources(\n",
    "        self,\n",
    "        sources\n",
    "    ):\n",
    "        \"\"\"Broadcast simple inputs to a dict\"\"\"\n",
    "        _sources = sources\n",
    "        if len(sources) == 1 and len(self._intervention_group) > 1:\n",
    "            for _ in range(len(self._intervention_group)-1):\n",
    "                _sources += [sources[0]]\n",
    "        else:\n",
    "            _sources = sources\n",
    "        return _sources\n",
    "    \n",
    "\n",
    "    def _broadcast_subspaces(\n",
    "        self,\n",
    "        batch_size,\n",
    "        subspaces\n",
    "    ):\n",
    "        \"\"\"Broadcast simple subspaces input\"\"\"\n",
    "        _subspaces = subspaces\n",
    "        if isinstance(subspaces, int):\n",
    "            _subspaces = [[[subspaces]]*batch_size]*len(self.interventions)\n",
    "            \n",
    "        elif isinstance(subspaces, list) and isinstance(subspaces[0], int):\n",
    "            _subspaces = [[subspaces]*batch_size]*len(self.interventions)\n",
    "        else:\n",
    "            # TODO: subspaces is easier to add more broadcast majic.\n",
    "            pass\n",
    "        return _subspaces\n",
    "    \n",
    "    def forward(self, **kwargs):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "\n",
    "    def generate(self, **kwargs):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "        \n",
    "\n",
    "class IntervenableNdifModel(BaseModel):\n",
    "    \"\"\"\n",
    "    Intervenable model via ndif backend.\n",
    "    \"\"\"\n",
    "    BACKEND = \"ndif\"\n",
    "    \n",
    "    def __init__(self, config, model, **kwargs):\n",
    "        super().__init__(config, model, \"ndif\", **kwargs)\n",
    "        # this is not used for now.\n",
    "        self.remote = kwargs[\"remote\"] if \"remote\" in kwargs else False\n",
    "        logging.warning(\n",
    "            f\"We currently have very limited intervention support for ndif backend.\"\n",
    "        )\n",
    "\n",
    "    def save(\n",
    "        self, save_directory, save_to_hf_hub=False, hf_repo_name=\"my-awesome-model\"\n",
    "    ):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def load(load_directory, model, local_directory=None, from_huggingface_hub=False):\n",
    "        \"\"\"\n",
    "        Load interventions from disk or hub\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _cleanup_states(self, skip_activation_gc=False):\n",
    "        \"\"\"\n",
    "        Clean up all old in memo states of interventions\n",
    "        \"\"\"\n",
    "        self._is_generation = False\n",
    "        if not skip_activation_gc:\n",
    "            self.activations.clear()\n",
    "            self.hot_activations.clear()\n",
    "            self._batched_setter_activation_select.clear()\n",
    "        else:\n",
    "            self.activations = {}\n",
    "            self.hot_activations = {}\n",
    "            self._batched_setter_activation_select = {}\n",
    "\n",
    "    def _tidy_stateful_activations(\n",
    "        self,\n",
    "    ):\n",
    "        _need_tidify = False\n",
    "\n",
    "    def _reconcile_stateful_cached_activations(\n",
    "        self,\n",
    "        key,\n",
    "        intervening_activations,\n",
    "        intervening_unit_locations,\n",
    "    ):\n",
    "        \"\"\"Based on the key, we consolidate activations based on key's state\"\"\"\n",
    "        if key not in self.activations:\n",
    "            return None\n",
    "\n",
    "        cached_activations = self.activations[key]\n",
    "        if self.is_model_stateless:\n",
    "            # nothing to reconcile if stateless\n",
    "            return cached_activations\n",
    "\n",
    "        raise NotImplementedError(\"Activation reconcile is not implemented for ndif backend\")\n",
    "    \n",
    "    def _intervention_getter(\n",
    "        self,\n",
    "        keys,\n",
    "        unit_locations,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create a list of getter handlers that will fetch activations\n",
    "        \"\"\"\n",
    "        handlers = []\n",
    "        for key_i, key in enumerate(keys):\n",
    "            intervention, (module_hook, hook_type) = self.interventions[key]\n",
    "            if self._is_generation:\n",
    "                raise NotImplementedError(\"Generation is not implemented for ndif backend\")\n",
    "\n",
    "            if hook_type == CONST_INPUT_HOOK:\n",
    "                output = module_hook.input\n",
    "            elif hook_type == CONST_OUTPUT_HOOK:\n",
    "                output = module_hook.output\n",
    "\n",
    "            # TODO: this could be faulty by assuming the types.\n",
    "            if isinstance(output.dtype, tuple) and isinstance(output.dtype[0], tuple):\n",
    "                output = output[0][0]\n",
    "            elif isinstance(output.dtype, tuple):\n",
    "                output = output[0]\n",
    "            \n",
    "            if isinstance(intervention, SkipIntervention):\n",
    "                raise NotImplementedError(\"Skip intervention is not implemented for ndif backend\")\n",
    "            else:\n",
    "                selected_output = self._gather_intervention_output(\n",
    "                    output, key, unit_locations[key_i]\n",
    "                )\n",
    "                print(selected_output)\n",
    "\n",
    "                if self.is_model_stateless:\n",
    "                    # WARNING: might be worth to check the below assertion at runtime,\n",
    "                    # but commenting it out for now just to avoid confusion.\n",
    "                    # assert key not in self.activations\n",
    "                    self.activations[key] = selected_output.save()\n",
    "                else:\n",
    "                    raise NotImplementedError(\"Stateful models are not supported for ndif backend\")\n",
    "\n",
    "                # set version for stateful models\n",
    "                self._intervention_state[key].inc_getter_version()\n",
    "\n",
    "    def _intervention_setter(\n",
    "        self,\n",
    "        keys,\n",
    "        unit_locations_base,\n",
    "        subspaces,\n",
    "    ) -> HandlerList:\n",
    "        \"\"\"\n",
    "        Create a list of setter tracer that will set activations\n",
    "        \"\"\"\n",
    "        self._tidy_stateful_activations()\n",
    "        \n",
    "        for key_i, key in enumerate(keys):\n",
    "            intervention, (module_hook, hook_type) = self.interventions[key]\n",
    "            if unit_locations_base[0] is not None:\n",
    "                self._batched_setter_activation_select[key] = [\n",
    "                    0 for _ in range(len(unit_locations_base[0]))\n",
    "                ]  # batch_size\n",
    "\n",
    "            if self._is_generation:\n",
    "                raise NotImplementedError(\"Generation is not implemented for ndif backend\")\n",
    "\n",
    "            if hook_type == CONST_INPUT_HOOK:\n",
    "                output = module_hook.input\n",
    "            elif hook_type == CONST_OUTPUT_HOOK:\n",
    "                output = module_hook.output\n",
    "\n",
    "            # TODO: this could be faulty by assuming the types.\n",
    "            if isinstance(output.dtype, tuple) and isinstance(output.dtype[0], tuple):\n",
    "                output = output[0][0]\n",
    "            elif isinstance(output.dtype, tuple):\n",
    "                output = output[0]\n",
    "\n",
    "            selected_output = self._gather_intervention_output(\n",
    "                output, key, unit_locations_base[key_i]\n",
    "            )\n",
    "            if not self.is_model_stateless:\n",
    "                raise NotImplementedError(\"Stateful models are not supported for ndif backend\")\n",
    "\n",
    "            # intervention in-place\n",
    "            if isinstance(\n",
    "                intervention,\n",
    "                CollectIntervention\n",
    "            ):\n",
    "                intervened_representation = do_intervention(\n",
    "                    selected_output,\n",
    "                    None,\n",
    "                    intervention,\n",
    "                    subspaces[key_i] if subspaces is not None else None,\n",
    "                )\n",
    "                # fail if this is not a fresh collect\n",
    "                assert key not in self.activations\n",
    "                self.activations[key] = intervened_representation.save()\n",
    "                # no-op to the output\n",
    "                \n",
    "            else:\n",
    "                if not isinstance(self.interventions[key][0], types.FunctionType):\n",
    "                    if intervention.is_source_constant:\n",
    "                        intervened_representation = do_intervention(\n",
    "                            selected_output,\n",
    "                            None,\n",
    "                            intervention,\n",
    "                            subspaces[key_i] if subspaces is not None else None,\n",
    "                        )\n",
    "                    else:\n",
    "                        intervened_representation = do_intervention(\n",
    "                            selected_output,\n",
    "                            self._reconcile_stateful_cached_activations(\n",
    "                                key,\n",
    "                                selected_output,\n",
    "                                unit_locations_base[key_i],\n",
    "                            ),\n",
    "                            intervention,\n",
    "                            subspaces[key_i] if subspaces is not None else None,\n",
    "                        )\n",
    "                else:\n",
    "                    # highly unlikely it's a primitive intervention type\n",
    "                    intervened_representation = do_intervention(\n",
    "                        selected_output,\n",
    "                        self._reconcile_stateful_cached_activations(\n",
    "                            key,\n",
    "                            selected_output,\n",
    "                            unit_locations_base[key_i],\n",
    "                        ),\n",
    "                        intervention,\n",
    "                        subspaces[key_i] if subspaces is not None else None,\n",
    "                    )\n",
    "                if intervened_representation is None:\n",
    "                    return\n",
    "\n",
    "                # setter can produce hot activations for shared subspace interventions if linked\n",
    "                if key in self._intervention_reverse_link:\n",
    "                    self.hot_activations[\n",
    "                        self._intervention_reverse_link[key]\n",
    "                    ] = intervened_representation.clone()\n",
    "                \n",
    "                if isinstance(output, tuple):\n",
    "                    _ = self._scatter_intervention_output(\n",
    "                        output[0], intervened_representation, key, unit_locations_base[key_i]\n",
    "                    )\n",
    "                else:\n",
    "                    _ = self._scatter_intervention_output(\n",
    "                        output, intervened_representation, key, unit_locations_base[key_i]\n",
    "                    )\n",
    "                        \n",
    "                self._intervention_state[key].inc_setter_version()\n",
    "    \n",
    "    def _sync_forward_with_parallel_intervention(\n",
    "        self,\n",
    "        base,\n",
    "        sources,\n",
    "        unit_locations,\n",
    "        activations_sources: Optional[Dict] = None,\n",
    "        subspaces: Optional[List] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # torch.autograd.set_detect_anomaly(True)\n",
    "        all_set_handlers = HandlerList([])\n",
    "        unit_locations_sources = unit_locations[\"sources->base\"][0]\n",
    "        unit_locations_base = unit_locations[\"sources->base\"][1]\n",
    "\n",
    "        # for each source, we hook in getters to cache activations\n",
    "        # at each aligning representations\n",
    "        if activations_sources is None:\n",
    "            assert len(sources) == len(self._intervention_group)\n",
    "            for group_id, keys in self._intervention_group.items():\n",
    "                if sources[group_id] is None:\n",
    "                    continue  # smart jump for advance usage only\n",
    "\n",
    "                # meta tracer to get activations for all components\n",
    "                with self.model.trace(sources[group_id]) as tracer:\n",
    "                    for key in keys:\n",
    "                        self._intervention_getter(\n",
    "                            [key],\n",
    "                            [\n",
    "                                unit_locations_sources[\n",
    "                                    self.sorted_keys.index(key)\n",
    "                                ]\n",
    "                            ],\n",
    "                        )\n",
    "                # upon exist, all activations should be saved\n",
    "        else:\n",
    "            # simply patch in the ones passed in\n",
    "            self.activations = activations_sources\n",
    "            for _, passed_in_key in enumerate(self.activations):\n",
    "                assert passed_in_key in self.sorted_keys\n",
    "        \n",
    "        # in parallel mode with ndif backend, we don't need to wait \n",
    "        # for the intervention hook, we synchronously do the interventions.\n",
    "        with self.model.trace(base, **kwargs) as tracer:\n",
    "            for group_id, keys in self._intervention_group.items():\n",
    "                for key in keys:\n",
    "                    # skip in case smart jump\n",
    "                    if key in self.activations or \\\n",
    "                        isinstance(self.interventions[key][0], types.FunctionType) or \\\n",
    "                        self.interventions[key][0].is_source_constant:\n",
    "                        self._intervention_setter(\n",
    "                            [key],\n",
    "                            [\n",
    "                                unit_locations_base[\n",
    "                                    self.sorted_keys.index(key)\n",
    "                                ]\n",
    "                            ],\n",
    "                            # assume same group targeting the same subspace\n",
    "                            [\n",
    "                                subspaces[\n",
    "                                    self.sorted_keys.index(key)\n",
    "                                ]\n",
    "                            ]\n",
    "                            if subspaces is not None\n",
    "                            else None,\n",
    "                        )\n",
    "            counterfactual_outputs = self.model.output.save()\n",
    "        \n",
    "        return counterfactual_outputs\n",
    "\n",
    "    def _sync_forward_with_serial_intervention(\n",
    "        self,\n",
    "        base,\n",
    "        sources,\n",
    "        unit_locations,\n",
    "        activations_sources: Optional[Dict] = None,\n",
    "        subspaces: Optional[List] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        raise NotImplementedError(\"Please Implement serial intervention support for ndif\")\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        base,\n",
    "        sources: Optional[List] = None,\n",
    "        unit_locations: Optional[Dict] = None,\n",
    "        source_representations: Optional[Dict] = None,\n",
    "        subspaces: Optional[List] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_original_output: Optional[bool] = False,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        use_cache: Optional[bool] = True,\n",
    "    ):\n",
    "        activations_sources = source_representations\n",
    "        if sources is not None and not isinstance(sources, list):\n",
    "            sources = [sources]\n",
    "        \n",
    "        self._cleanup_states()\n",
    "\n",
    "        # if no source input or intervention, we return base\n",
    "        if sources is None and activations_sources is None \\\n",
    "            and unit_locations is None and len(self.interventions) == 0:\n",
    "            # ndif backend call\n",
    "            with self.model.trace(base) as tracer:\n",
    "                base_outputs = self.model.output.save()\n",
    "            return base_outputs, None\n",
    "        # broadcast\n",
    "        unit_locations = self._broadcast_unit_locations(get_batch_size(base), unit_locations)\n",
    "        sources = [None]*len(self._intervention_group) if sources is None else sources\n",
    "        sources = self._broadcast_sources(sources)\n",
    "        activations_sources = self._broadcast_source_representations(activations_sources)\n",
    "        subspaces = self._broadcast_subspaces(get_batch_size(base), subspaces)\n",
    "        \n",
    "        self._input_validation(\n",
    "            base,\n",
    "            sources,\n",
    "            unit_locations,\n",
    "            activations_sources,\n",
    "            subspaces,\n",
    "        )\n",
    "        \n",
    "        base_outputs = None\n",
    "        if output_original_output:\n",
    "            # returning un-intervened output with gradients with ndif backend call\n",
    "            with self.model.trace(base) as tracer:\n",
    "                base_outputs = self.model.output.save()\n",
    "\n",
    "        # intervene the model based on ndif APIs\n",
    "        try:\n",
    "\n",
    "            # run intervened forward\n",
    "            model_kwargs = {}\n",
    "            if labels is not None: # for training\n",
    "                model_kwargs[\"labels\"] = labels\n",
    "            if 'use_cache' in self.model.config.to_dict(): # for transformer models\n",
    "                model_kwargs[\"use_cache\"] = use_cache\n",
    "\n",
    "            if self.mode == \"parallel\":\n",
    "                counterfactual_outputs = self._sync_forward_with_parallel_intervention(\n",
    "                    base,\n",
    "                    sources,\n",
    "                    unit_locations,\n",
    "                    activations_sources,\n",
    "                    subspaces,\n",
    "                    **model_kwargs,\n",
    "                )\n",
    "            elif self.mode == \"serial\":\n",
    "                counterfactual_outputs = self._sync_forward_with_serial_intervention(\n",
    "                    base,\n",
    "                    sources,\n",
    "                    unit_locations,\n",
    "                    activations_sources,\n",
    "                    subspaces,\n",
    "                    **model_kwargs,\n",
    "                )\n",
    "            \n",
    "            collected_activations = []\n",
    "            if self.return_collect_activations:\n",
    "                for key in self.sorted_keys:\n",
    "                    if isinstance(\n",
    "                        self.interventions[key][0],\n",
    "                        CollectIntervention\n",
    "                    ):\n",
    "                        collected_activations += self.activations[key].clone()\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        finally:\n",
    "            self._cleanup_states(\n",
    "                skip_activation_gc = \\\n",
    "                    (sources is None and activations_sources is not None) or \\\n",
    "                    self.return_collect_activations\n",
    "            )\n",
    "        \n",
    "        if self.return_collect_activations:\n",
    "            if return_dict:\n",
    "                return IntervenableModelOutput(\n",
    "                    original_outputs=base_outputs,\n",
    "                    intervened_outputs=counterfactual_outputs,\n",
    "                    collected_activations=collected_activations\n",
    "                )\n",
    "            \n",
    "            return (base_outputs, collected_activations), counterfactual_outputs\n",
    "        \n",
    "        if return_dict:\n",
    "            return IntervenableModelOutput(\n",
    "                original_outputs=base_outputs,\n",
    "                intervened_outputs=counterfactual_outputs,\n",
    "                collected_activations=None\n",
    "            )\n",
    "\n",
    "        return base_outputs, counterfactual_outputs\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        base,\n",
    "        sources: Optional[List] = None,\n",
    "        unit_locations: Optional[Dict] = None,\n",
    "        source_representations: Optional[Dict] = None,\n",
    "        intervene_on_prompt: bool = False,\n",
    "        subspaces: Optional[List] = None,\n",
    "        output_original_output: Optional[bool] = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "\n",
    "\n",
    "\n",
    "class IntervenableModel(BaseModel):\n",
    "    \"\"\"\n",
    "    Intervenable model via pyvene native backend (hook-based).\n",
    "    \"\"\"\n",
    "    BACKEND = \"native\"\n",
    "    \n",
    "    def __init__(self, config, model, **kwargs):\n",
    "        super().__init__(config, model, \"native\", **kwargs)\n",
    "\n",
    "\n",
    "    def _reset_hook_count(self):\n",
    "        \"\"\"\n",
    "        Reset the hook count before any generate call\n",
    "        \"\"\"\n",
    "        self._key_getter_call_counter = dict.fromkeys(self._key_getter_call_counter, 0)\n",
    "        self._key_setter_call_counter = dict.fromkeys(self._key_setter_call_counter, 0)\n",
    "        for k, _ in self._intervention_state.items():\n",
    "            self._intervention_state[k].reset()\n",
    "\n",
    "\n",
    "    def _remove_forward_hooks(self):\n",
    "        \"\"\"\n",
    "        Clean up all the remaining hooks before any call\n",
    "        \"\"\"\n",
    "        remove_forward_hooks(self.model)\n",
    "\n",
    "\n",
    "    def _cleanup_states(self, skip_activation_gc=False):\n",
    "        \"\"\"\n",
    "        Clean up all old in memo states of interventions\n",
    "        \"\"\"\n",
    "        self._is_generation = False\n",
    "        self._remove_forward_hooks()\n",
    "        self._reset_hook_count()\n",
    "        if not skip_activation_gc:\n",
    "            self.activations.clear()\n",
    "            self.hot_activations.clear()\n",
    "            self._batched_setter_activation_select.clear()\n",
    "        else:\n",
    "            self.activations = {}\n",
    "            self.hot_activations = {}\n",
    "            self._batched_setter_activation_select = {}\n",
    "    \n",
    "\n",
    "    def save(\n",
    "        self, save_directory, save_to_hf_hub=False, hf_repo_name=\"my-awesome-model\",\n",
    "        include_model=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Save interventions to disk or hub\n",
    "        \"\"\"\n",
    "        if save_to_hf_hub:\n",
    "            from huggingface_hub import HfApi\n",
    "\n",
    "            api = HfApi()\n",
    "\n",
    "        create_directory(save_directory)\n",
    "\n",
    "        saving_config = copy.deepcopy(self.config)\n",
    "        saving_config.sorted_keys = self.sorted_keys\n",
    "        saving_config.model_type = str(\n",
    "            saving_config.model_type\n",
    "        )\n",
    "        saving_config.intervention_types = []\n",
    "        saving_config.intervention_dimensions = []\n",
    "        saving_config.intervention_constant_sources = []\n",
    "        \n",
    "        # handle constant source reprs if passed in.\n",
    "        serialized_representations = []\n",
    "        for reprs in saving_config.representations:\n",
    "            serialized_reprs = {}\n",
    "            for k, v in reprs._asdict().items():\n",
    "                if k == \"hidden_source_representation\":\n",
    "                    continue\n",
    "                if k == \"source_representation\":\n",
    "                    # hidden flag only set here\n",
    "                    if v is not None:\n",
    "                        serialized_reprs[\"hidden_source_representation\"] = True\n",
    "                    serialized_reprs[k] = None\n",
    "                elif k == \"intervention_type\":\n",
    "                    serialized_reprs[k] = None\n",
    "                elif k == \"intervention\":\n",
    "                    serialized_reprs[k] = None\n",
    "                else:\n",
    "                    serialized_reprs[k] = v\n",
    "            serialized_representations += [\n",
    "                RepresentationConfig(**serialized_reprs)\n",
    "            ]\n",
    "        saving_config.representations = \\\n",
    "            serialized_representations\n",
    "        \n",
    "        for k, v in self.interventions.items():\n",
    "            intervention = v[0]\n",
    "            saving_config.intervention_types += [str(type(intervention))]\n",
    "            binary_filename = f\"intkey_{k}.bin\"\n",
    "            # save intervention binary file\n",
    "            if isinstance(intervention, TrainableIntervention) or \\\n",
    "                intervention.source_representation is not None:\n",
    "                # logging.info(f\"Saving trainable intervention to {binary_filename}.\")\n",
    "                torch.save(\n",
    "                    intervention.state_dict(),\n",
    "                    os.path.join(save_directory, binary_filename),\n",
    "                )\n",
    "                if save_to_hf_hub:\n",
    "                    # push to huggingface hub\n",
    "                    try:\n",
    "                        api.create_repo(hf_repo_name)\n",
    "                    except:\n",
    "                        logging.info(\n",
    "                            f\"Uploading: {binary_filename}, but skipping creating the repo since \"\n",
    "                            f\"either {hf_repo_name} exists or having authentication error.\"\n",
    "                        )\n",
    "                    api.upload_file(\n",
    "                        path_or_fileobj=os.path.join(save_directory, binary_filename),\n",
    "                        path_in_repo=binary_filename,\n",
    "                        repo_id=hf_repo_name,\n",
    "                        repo_type=\"model\",\n",
    "                    )\n",
    "            if intervention.interchange_dim is None:\n",
    "                saving_config.intervention_dimensions += [None]\n",
    "            else:\n",
    "                saving_config.intervention_dimensions += [intervention.interchange_dim.tolist()]\n",
    "            saving_config.intervention_constant_sources += [intervention.is_source_constant]\n",
    "\n",
    "        # save model's trainable parameters as well\n",
    "        if include_model:\n",
    "            model_state_dict = {}\n",
    "            model_binary_filename = \"pytorch_model.bin\"\n",
    "            for n, p in self.model.named_parameters():\n",
    "                if p.requires_grad:\n",
    "                    model_state_dict[n] = p\n",
    "            torch.save(model_state_dict, os.path.join(save_directory, model_binary_filename))\n",
    "            \n",
    "        # save metadata config\n",
    "        saving_config.save_pretrained(save_directory)\n",
    "        if save_to_hf_hub:\n",
    "            # push to huggingface hub\n",
    "            try:\n",
    "                api.create_repo(hf_repo_name)\n",
    "            except:\n",
    "                logging.info(\n",
    "                    f\"Uploading the config, Skipping creating the repo since \"\n",
    "                    f\"either {hf_repo_name} exists or having authentication error.\"\n",
    "                )\n",
    "            api.upload_file(\n",
    "                path_or_fileobj=os.path.join(save_directory, \"config.json\"),\n",
    "                path_in_repo=\"config.json\",\n",
    "                repo_id=hf_repo_name,\n",
    "                repo_type=\"model\",\n",
    "            )\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load(\n",
    "        load_directory, model, local_directory=None, from_huggingface_hub=False,\n",
    "        include_model=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Load interventions from disk or hub\n",
    "        \"\"\"\n",
    "        if not os.path.exists(load_directory) or from_huggingface_hub:\n",
    "            from_huggingface_hub = True\n",
    "            \n",
    "            from huggingface_hub import snapshot_download\n",
    "            load_directory = snapshot_download(\n",
    "                repo_id=load_directory,\n",
    "                local_dir=local_directory,\n",
    "            )\n",
    "\n",
    "        # load config\n",
    "        saving_config = IntervenableConfig.from_pretrained(load_directory)\n",
    "        casted_intervention_types = []\n",
    "\n",
    "        for type_str in saving_config.intervention_types:\n",
    "            casted_intervention_types += [get_type_from_string(type_str)]\n",
    "        saving_config.intervention_types = (\n",
    "            casted_intervention_types\n",
    "        )\n",
    "        casted_representations = []\n",
    "        for (\n",
    "            representation_opts\n",
    "        ) in saving_config.representations:\n",
    "            casted_representations += [\n",
    "                RepresentationConfig(*representation_opts)\n",
    "            ]\n",
    "        saving_config.representations = casted_representations\n",
    "        intervenable = IntervenableModel(saving_config, model)\n",
    "\n",
    "        # load binary files\n",
    "        for i, (k, v) in enumerate(intervenable.interventions.items()):\n",
    "            intervention = v[0]\n",
    "            binary_filename = f\"intkey_{k}.bin\"\n",
    "            intervention.is_source_constant = \\\n",
    "                saving_config.intervention_constant_sources[i]\n",
    "            intervention.set_interchange_dim(saving_config.intervention_dimensions[i])\n",
    "            if saving_config.intervention_constant_sources[i] and \\\n",
    "                not isinstance(intervention, ZeroIntervention) and \\\n",
    "                not isinstance(intervention, SourcelessIntervention):\n",
    "                # logging.warn(f\"Loading trainable intervention from {binary_filename}.\")\n",
    "                saved_state_dict = torch.load(os.path.join(load_directory, binary_filename))\n",
    "                try:\n",
    "                    intervention.register_buffer(\n",
    "                        'source_representation', saved_state_dict['source_representation']\n",
    "                    )\n",
    "                except:\n",
    "                    intervention.source_representation = saved_state_dict['source_representation']\n",
    "            elif isinstance(intervention, TrainableIntervention):\n",
    "                saved_state_dict = torch.load(os.path.join(load_directory, binary_filename))\n",
    "                intervention.load_state_dict(saved_state_dict)\n",
    "\n",
    "        # load model's trainable parameters as well\n",
    "        if include_model:\n",
    "            model_binary_filename = \"pytorch_model.bin\"\n",
    "            saved_model_state_dict = torch.load(os.path.join(load_directory, model_binary_filename))\n",
    "            intervenable.model.load_state_dict(saved_model_state_dict, strict=False)\n",
    "\n",
    "        return intervenable\n",
    "\n",
    "\n",
    "    def save_intervention(self, save_directory, include_model=True):\n",
    "        \"\"\"\n",
    "        Instead of saving the metadata with artifacts, it only saves artifacts such as\n",
    "        trainable weights. This is not a static method, and returns nothing.\n",
    "        \"\"\"\n",
    "        create_directory(save_directory)\n",
    "        \n",
    "        # save binary files\n",
    "        for k, v in self.interventions.items():\n",
    "            intervention = v[0]\n",
    "            binary_filename = f\"intkey_{k}.bin\"\n",
    "            # save intervention binary file\n",
    "            if isinstance(intervention, TrainableIntervention):\n",
    "                torch.save(intervention.state_dict(),\n",
    "                    os.path.join(save_directory, binary_filename))\n",
    "\n",
    "        # save model's trainable parameters as well\n",
    "        if include_model:\n",
    "            model_state_dict = {}\n",
    "            model_binary_filename = \"pytorch_model.bin\"\n",
    "            for n, p in self.model.named_parameters():\n",
    "                if p.requires_grad:\n",
    "                    model_state_dict[n] = p\n",
    "            torch.save(model_state_dict, os.path.join(save_directory, model_binary_filename))\n",
    "    \n",
    "\n",
    "    def load_intervention(self, load_directory, include_model=True):\n",
    "        \"\"\"\n",
    "        Instead of creating an new object, this function loads existing weights onto\n",
    "        the current object. This is not a static method, and returns nothing.\n",
    "        \"\"\"\n",
    "        # load binary files\n",
    "        for i, (k, v) in enumerate(self.interventions.items()):\n",
    "            intervention = v[0]\n",
    "            binary_filename = f\"intkey_{k}.bin\"\n",
    "            if isinstance(intervention, TrainableIntervention):\n",
    "                saved_state_dict = torch.load(os.path.join(load_directory, binary_filename))\n",
    "                intervention.load_state_dict(saved_state_dict)\n",
    "\n",
    "        # load model's trainable parameters as well\n",
    "        if include_model:\n",
    "            model_binary_filename = \"pytorch_model.bin\"\n",
    "            saved_model_state_dict = torch.load(os.path.join(load_directory, model_binary_filename))\n",
    "            self.model.load_state_dict(saved_model_state_dict, strict=False)\n",
    "\n",
    "\n",
    "    def _tidy_stateful_activations(\n",
    "        self,\n",
    "    ):\n",
    "        _need_tidify = False\n",
    "        for _, v in self.activations.items():\n",
    "            if isinstance(v[0], tuple) and isinstance(v[0][1], list):\n",
    "                _need_tidify = True\n",
    "                break\n",
    "        if _need_tidify:\n",
    "            for k, v in self.activations.items():\n",
    "                self._tidify_activations = [[] for _ in range(v[0][0].shape[0])]\n",
    "                for t in range(len(v)):\n",
    "                    activations_at_t = v[t][0]  # a batched tensor\n",
    "                    states_at_t = (\n",
    "                        torch.tensor(v[t][1]).bool().to(activations_at_t.device)\n",
    "                    )  # a batched bools\n",
    "                    selected_activations = activations_at_t[states_at_t]\n",
    "                    selected_indices = torch.nonzero(states_at_t).squeeze()\n",
    "                    if len(selected_indices.shape) == 0:\n",
    "                        selected_indices = selected_indices.unsqueeze(0)\n",
    "                    for index, activation in zip(\n",
    "                        selected_indices, selected_activations\n",
    "                    ):\n",
    "                        self._tidify_activations[index].append(activation)\n",
    "                self.activations[k] = self._tidify_activations\n",
    "\n",
    "\n",
    "    def _reconcile_stateful_cached_activations(\n",
    "        self,\n",
    "        key,\n",
    "        intervening_activations,\n",
    "        intervening_unit_locations,\n",
    "    ):\n",
    "        \"\"\"Based on the key, we consolidate activations based on key's state\"\"\"\n",
    "        if key not in self.activations:\n",
    "            return None\n",
    "\n",
    "        cached_activations = self.activations[key]\n",
    "        if self.is_model_stateless:\n",
    "            # nothing to reconcile if stateless\n",
    "            return cached_activations\n",
    "\n",
    "        state_select_flag = []\n",
    "        for unit_location in intervening_unit_locations:\n",
    "            if self._intervention_state[key].setter_version() in unit_location:\n",
    "                state_select_flag += [True]\n",
    "            else:\n",
    "                state_select_flag += [False]\n",
    "        state_select_flag = (\n",
    "            torch.tensor(state_select_flag).bool().to(intervening_activations.device)\n",
    "        )\n",
    "        selected_indices = torch.nonzero(state_select_flag).squeeze()\n",
    "        if len(selected_indices.shape) == 0:\n",
    "            selected_indices = selected_indices.unsqueeze(0)\n",
    "\n",
    "        # fill activations with proposed only source activations\n",
    "        reconciled_activations = []\n",
    "        for index, select_version in enumerate(\n",
    "            self._batched_setter_activation_select[key]\n",
    "        ):\n",
    "            if index in selected_indices:\n",
    "                reconciled_activations += [cached_activations[index][select_version]]\n",
    "            else:\n",
    "                # WARNING: put a dummy tensor, super danger here but let's trust the code for now.\n",
    "                reconciled_activations += [\n",
    "                    torch.zeros_like(cached_activations[index][0])\n",
    "                ]\n",
    "        # increment pointer for those we are actually intervening\n",
    "        for index in selected_indices:\n",
    "            self._batched_setter_activation_select[key][index] += 1\n",
    "        # for non-intervening ones, we copy again from base\n",
    "        reconciled_activations = torch.stack(reconciled_activations, dim=0)  # batched\n",
    "        # reconciled_activations[~state_select_flag] = intervening_activations[~state_select_flag]\n",
    "\n",
    "        return reconciled_activations\n",
    "    \n",
    "\n",
    "    def _intervention_getter(\n",
    "        self,\n",
    "        keys,\n",
    "        unit_locations,\n",
    "    ) -> HandlerList:\n",
    "        \"\"\"\n",
    "        Create a list of getter handlers that will fetch activations\n",
    "        \"\"\"\n",
    "        handlers = []\n",
    "\n",
    "        print(keys)\n",
    "        print('')\n",
    "        \n",
    "        for key_i, key in enumerate(keys):\n",
    "            intervention, module_hook = self.interventions[key]\n",
    "\n",
    "            def hook_callback(model, args, kwargs, output=None):\n",
    "                if self._is_generation:\n",
    "                    pass\n",
    "                    # for getter, there is no restriction.\n",
    "                    # is_prompt = self._key_getter_call_counter[key] == 0\n",
    "                    # if not self._intervene_on_prompt or is_prompt:\n",
    "                    #     self._key_getter_call_counter[key] += 1\n",
    "                    # if self._intervene_on_prompt ^ is_prompt:\n",
    "                    #     return  # no-op\n",
    "                if output is None:\n",
    "                    if len(args) == 0:  # kwargs based calls\n",
    "                        # PR: https://github.com/frankaging/align-transformers/issues/11\n",
    "                        # We cannot assume the dict only contain one element\n",
    "                        output = kwargs[list(kwargs.keys())[0]]\n",
    "                    else:\n",
    "                        output = args\n",
    "\n",
    "                if isinstance(intervention, SkipIntervention):\n",
    "                    selected_output = self._gather_intervention_output(\n",
    "                        args[0],  # this is actually the input to the module\n",
    "                        key,\n",
    "                        unit_locations[key_i],\n",
    "                    )\n",
    "                else:\n",
    "                    selected_output = self._gather_intervention_output(\n",
    "                        output, key, unit_locations[key_i]\n",
    "                    )\n",
    "\n",
    "                if self.is_model_stateless:\n",
    "                    # WARNING: might be worth to check the below assertion at runtime,\n",
    "                    # but commenting it out for now just to avoid confusion.\n",
    "                    # assert key not in self.activations\n",
    "                    self.activations[key] = selected_output\n",
    "                else:\n",
    "                    state_select_flag = []\n",
    "                    for unit_location in unit_locations[key_i]:\n",
    "                        if (\n",
    "                            self._intervention_state[key].getter_version()\n",
    "                            in unit_location\n",
    "                        ):\n",
    "                            state_select_flag += [True]\n",
    "                        else:\n",
    "                            state_select_flag += [False]\n",
    "                    # for stateful model (e.g., gru), we save extra activations and metadata to do\n",
    "                    # stateful interventions.\n",
    "                    self.activations.setdefault(key, []).append(\n",
    "                        (selected_output, state_select_flag)\n",
    "                    )\n",
    "                # set version for stateful models\n",
    "                self._intervention_state[key].inc_getter_version()\n",
    "\n",
    "            handlers.append(module_hook(hook_callback, with_kwargs=True))\n",
    "\n",
    "        return HandlerList(handlers)\n",
    "\n",
    "\n",
    "    def _intervention_setter(\n",
    "        self,\n",
    "        keys,\n",
    "        unit_locations_base,\n",
    "        subspaces,\n",
    "    ) -> HandlerList:\n",
    "        \"\"\"\n",
    "        Create a list of setter handlers that will set activations\n",
    "        \"\"\"\n",
    "        self._tidy_stateful_activations()\n",
    "\n",
    "        print(keys)\n",
    "        raise\n",
    "        \n",
    "        handlers = []\n",
    "        for key_i, key in enumerate(keys):\n",
    "            intervention, module_hook = self.interventions[key]\n",
    "            if unit_locations_base[0] is not None:\n",
    "                self._batched_setter_activation_select[key] = [\n",
    "                    0 for _ in range(len(unit_locations_base[0]))\n",
    "                ]  # batch_size\n",
    "\n",
    "            def hook_callback(model, args, kwargs, output=None):\n",
    "                if self._is_generation:\n",
    "                    is_prompt = self._key_setter_call_counter[key] == 0\n",
    "                    if not self._intervene_on_prompt or is_prompt:\n",
    "                        self._key_setter_call_counter[key] += 1\n",
    "                    if self._intervene_on_prompt ^ is_prompt:\n",
    "                        return  # no-op\n",
    "                if output is None:\n",
    "                    if len(args) == 0:  # kwargs based calls\n",
    "                        # PR: https://github.com/frankaging/align-transformers/issues/11\n",
    "                        # We cannot assume the dict only contain one element\n",
    "                        output = kwargs[list(kwargs.keys())[0]]\n",
    "                    else:\n",
    "                        output = args\n",
    "                        \n",
    "                selected_output = self._gather_intervention_output(\n",
    "                    output, key, unit_locations_base[key_i]\n",
    "                )\n",
    "                # TODO: need to figure out why clone is needed\n",
    "                if not self.is_model_stateless:\n",
    "                    selected_output = selected_output.clone()\n",
    "                \n",
    "                if isinstance(\n",
    "                    intervention,\n",
    "                    CollectIntervention\n",
    "                ):\n",
    "                    intervened_representation = do_intervention(\n",
    "                        selected_output,\n",
    "                        None,\n",
    "                        intervention,\n",
    "                        subspaces[key_i] if subspaces is not None else None,\n",
    "                    )\n",
    "                    # fail if this is not a fresh collect\n",
    "                    assert key not in self.activations\n",
    "                    \n",
    "                    self.activations[key] = intervened_representation\n",
    "                    # no-op to the output\n",
    "                    \n",
    "                else:\n",
    "                    if not isinstance(self.interventions[key][0], types.FunctionType):\n",
    "                        if intervention.is_source_constant:\n",
    "                            intervened_representation = do_intervention(\n",
    "                                selected_output,\n",
    "                                None,\n",
    "                                intervention,\n",
    "                                subspaces[key_i] if subspaces is not None else None,\n",
    "                            )\n",
    "                        else:\n",
    "                            intervened_representation = do_intervention(\n",
    "                                selected_output,\n",
    "                                self._reconcile_stateful_cached_activations(\n",
    "                                    key,\n",
    "                                    selected_output,\n",
    "                                    unit_locations_base[key_i],\n",
    "                                ),\n",
    "                                intervention,\n",
    "                                subspaces[key_i] if subspaces is not None else None,\n",
    "                            )\n",
    "                    else:\n",
    "                        # highly unlikely it's a primitive intervention type\n",
    "                        intervened_representation = do_intervention(\n",
    "                            selected_output,\n",
    "                            self._reconcile_stateful_cached_activations(\n",
    "                                key,\n",
    "                                selected_output,\n",
    "                                unit_locations_base[key_i],\n",
    "                            ),\n",
    "                            intervention,\n",
    "                            subspaces[key_i] if subspaces is not None else None,\n",
    "                        )\n",
    "                    if intervened_representation is None:\n",
    "                        return\n",
    "\n",
    "                    # setter can produce hot activations for shared subspace interventions if linked\n",
    "                    if key in self._intervention_reverse_link:\n",
    "                        self.hot_activations[\n",
    "                            self._intervention_reverse_link[key]\n",
    "                        ] = intervened_representation.clone()\n",
    "\n",
    "                    if isinstance(output, tuple):\n",
    "                        _ = self._scatter_intervention_output(\n",
    "                            output[0], intervened_representation, key, unit_locations_base[key_i]\n",
    "                        )\n",
    "                    else:\n",
    "                        _ = self._scatter_intervention_output(\n",
    "                            output, intervened_representation, key, unit_locations_base[key_i]\n",
    "                        )\n",
    "                            \n",
    "                    self._intervention_state[key].inc_setter_version()\n",
    "\n",
    "            # ex : register_forward_pre_hook\n",
    "            # https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "            # with_kwargs : If true, the hook will be passed the kwargs given to forward\n",
    "            handlers.append(module_hook(hook_callback, with_kwargs=True))\n",
    "\n",
    "        return HandlerList(handlers)\n",
    "\n",
    "\n",
    "    def _output_validation(\n",
    "        self,\n",
    "    ):\n",
    "        \"\"\"Safe guarding the execution by checking memory states\"\"\"\n",
    "        if self.is_model_stateless:\n",
    "            for k, v in self._intervention_state.items():\n",
    "                if v.getter_version() > 1 or v.setter_version() > 1:\n",
    "                    raise Exception(\n",
    "                        f\"For stateless model, each getter and setter \"\n",
    "                        f\"should be called only once: {self._intervention_state}\"\n",
    "                    )\n",
    "\n",
    "    def _flatten_input_dict_as_batch(self, input_dict):\n",
    "        # we also accept grouped sources, will batch them and provide partition info.\n",
    "        if not isinstance(input_dict, dict):\n",
    "            assert isinstance(input_dict, list)\n",
    "            flatten_input_dict = {}\n",
    "            for k, v in input_dict[0].items():\n",
    "                flatten_input_dict[k] = {}\n",
    "            for i in range(0, len(input_dict)):\n",
    "                for k, v in input_dict[i].items():\n",
    "                    flatten_input_dict[k] += [v]\n",
    "            for k, v in flatten_input_dict.items():\n",
    "                # flatten as one single batch\n",
    "                flatten_input_dict[k] = torch.cat(v, dim=0)\n",
    "        else:\n",
    "            flatten_input_dict = input_dict\n",
    "        return flatten_input_dict\n",
    "\n",
    "\n",
    "    def _get_partition_size(self, input_dict):\n",
    "        if not isinstance(input_dict, dict):\n",
    "            assert isinstance(input_dict, list)\n",
    "            return len(input_dict)\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "\n",
    "    def _wait_for_forward_with_parallel_intervention(\n",
    "        self,\n",
    "        sources,\n",
    "        unit_locations,\n",
    "        activations_sources: Optional[Dict] = None,\n",
    "        subspaces: Optional[List] = None,\n",
    "    ):\n",
    "        # torch.autograd.set_detect_anomaly(True)\n",
    "        all_set_handlers = HandlerList([])\n",
    "        unit_locations_sources = unit_locations[\"sources->base\"][0]\n",
    "        unit_locations_base = unit_locations[\"sources->base\"][1]\n",
    "\n",
    "        # for each source, we hook in getters to cache activations\n",
    "        # at each aligning representations\n",
    "        \n",
    "        if activations_sources is None:\n",
    "            assert len(sources) == len(self._intervention_group)\n",
    "            for group_id, keys in self._intervention_group.items():\n",
    "                if sources[group_id] is None:\n",
    "                    continue  # smart jump for advance usage only\n",
    "                group_get_handlers = HandlerList([])\n",
    "                for key in keys:\n",
    "                    get_handlers = self._intervention_getter(\n",
    "                        [key],\n",
    "                        [unit_locations_sources[self.sorted_keys.index(key)]],\n",
    "                    )\n",
    "                    group_get_handlers.extend(get_handlers)\n",
    "                _ = self.model(**sources[group_id])\n",
    "                group_get_handlers.remove()\n",
    "        else:\n",
    "            # simply patch in the ones passed in\n",
    "            self.activations = activations_sources\n",
    "            for _, passed_in_key in enumerate(self.activations):\n",
    "                assert passed_in_key in self.sorted_keys\n",
    "        \n",
    "        # in parallel mode, we swap cached activations all into\n",
    "        # base at once\n",
    "        for group_id, keys in self._intervention_group.items():\n",
    "            for key in keys:\n",
    "                # skip in case smart jump\n",
    "                if key in self.activations or \\\n",
    "                    isinstance(self.interventions[key][0], types.FunctionType) or \\\n",
    "                    self.interventions[key][0].is_source_constant:\n",
    "                    set_handlers = self._intervention_setter(\n",
    "                        [key],\n",
    "                        [\n",
    "                            unit_locations_base[\n",
    "                                self.sorted_keys.index(key)\n",
    "                            ]\n",
    "                        ],\n",
    "                        # assume same group targeting the same subspace\n",
    "                        [\n",
    "                            subspaces[\n",
    "                                self.sorted_keys.index(key)\n",
    "                            ]\n",
    "                        ]\n",
    "                        if subspaces is not None\n",
    "                        else None,\n",
    "                    )\n",
    "                    # for setters, we don't remove them.\n",
    "                    all_set_handlers.extend(set_handlers)\n",
    "\n",
    "        return all_set_handlers\n",
    "\n",
    "\n",
    "    def _wait_for_forward_with_serial_intervention(\n",
    "        self,\n",
    "        sources,\n",
    "        unit_locations,\n",
    "        activations_sources: Optional[Dict] = None,\n",
    "        subspaces: Optional[List] = None,\n",
    "    ):\n",
    "        all_set_handlers = HandlerList([])\n",
    "        for group_id, keys in self._intervention_group.items():\n",
    "            if sources[group_id] is None:\n",
    "                continue  # smart jump for advance usage only\n",
    "            for key_id, key in enumerate(keys):\n",
    "                if group_id != len(self._intervention_group) - 1:\n",
    "                    unit_locations_key = f\"source_{group_id}->source_{group_id+1}\"\n",
    "                else:\n",
    "                    unit_locations_key = f\"source_{group_id}->base\"\n",
    "                unit_locations_source = unit_locations[unit_locations_key][0][\n",
    "                    key_id\n",
    "                ]\n",
    "                if unit_locations_source is None:\n",
    "                    continue  # smart jump for advance usage only\n",
    "\n",
    "                unit_locations_base = unit_locations[unit_locations_key][1][\n",
    "                    key_id\n",
    "                ]\n",
    "                if activations_sources is None:\n",
    "                    # get activation from source_i\n",
    "                    get_handlers = self._intervention_getter(\n",
    "                        [key],\n",
    "                        [unit_locations_source],\n",
    "                    )\n",
    "                else:\n",
    "                    self.activations[key] = activations_sources[\n",
    "                        key\n",
    "                    ]\n",
    "            # call once per group. each intervention is by its own group by default\n",
    "            if activations_sources is None:\n",
    "                # this is when previous setter and THEN the getter get called\n",
    "                _ = self.model(**sources[group_id])\n",
    "                get_handlers.remove()\n",
    "                # remove existing setters after getting the curr intervened reprs\n",
    "                if len(all_set_handlers) > 0:\n",
    "                    all_set_handlers.remove()\n",
    "                    all_set_handlers = HandlerList([])\n",
    "\n",
    "            for key in keys:\n",
    "                # skip in case smart jump\n",
    "                if key in self.activations or \\\n",
    "                    isinstance(self.interventions[key][0], types.FunctionType) or \\\n",
    "                    self.interventions[key][0].is_source_constant:\n",
    "                    # set with intervened activation to source_i+1\n",
    "                    set_handlers = self._intervention_setter(\n",
    "                        [key],\n",
    "                        [unit_locations_base],\n",
    "                        # assume the order\n",
    "                        [\n",
    "                            subspaces[\n",
    "                                self.sorted_keys.index(key)\n",
    "                            ]\n",
    "                        ]\n",
    "                        if subspaces is not None\n",
    "                        else None,\n",
    "                    )\n",
    "                    # for setters, we don't remove them.\n",
    "                    all_set_handlers.extend(set_handlers)\n",
    "        return all_set_handlers\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        base,\n",
    "        sources: Optional[List] = None,\n",
    "        unit_locations: Optional[Dict] = None,\n",
    "        source_representations: Optional[Dict] = None,\n",
    "        subspaces: Optional[List] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_original_output: Optional[bool] = False,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        use_cache: Optional[bool] = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Main forward function that serves a wrapper to\n",
    "        actual model forward calls. It will use forward\n",
    "        hooks to do interventions.\n",
    "\n",
    "        In essense, sources will lead to getter hooks to\n",
    "        get activations. We will use these activations to\n",
    "        intervene on our base example.\n",
    "\n",
    "        Parameters:\n",
    "        base:                The base example.\n",
    "        sources:             A list of source examples.\n",
    "        unit_locations:      The intervention locations.\n",
    "        activations_sources: A list of representations.\n",
    "        subspace:            Subspace interventions.\n",
    "\n",
    "        Return:\n",
    "        base_output: the non-intervened output of the base\n",
    "        input.\n",
    "        counterfactual_outputs: the intervened output of the\n",
    "        base input.\n",
    "\n",
    "        Notes:\n",
    "\n",
    "        1) unit_locations\n",
    "        unit_locations is a dict where keys are tied with\n",
    "        example pairs involved in one intervention as,\n",
    "        {\n",
    "            \"sources->base\" : List[]\n",
    "        }\n",
    "\n",
    "        the shape can be\n",
    "\n",
    "        2 * num_intervention * bs * num_max_unit\n",
    "\n",
    "        OR\n",
    "\n",
    "        2 * num_intervention * num_intervention_level * bs * num_max_unit\n",
    "\n",
    "        if we intervene on h.pos which is a nested intervention location.\n",
    "\n",
    "        2) subspaces\n",
    "        subspaces is a list of indices indicating which subspace will\n",
    "        this intervention target given an example in the batch.\n",
    "\n",
    "        An intervention could be initialized with subspace parition as,\n",
    "        [[... subspace_1 ...], [... subspace_2 ...], [rest]]\n",
    "\n",
    "        An intervention may be targeting a specific partition.\n",
    "\n",
    "        This input field should look like something like,\n",
    "        [\n",
    "            [[subspace indices], [subspace indices]], <- for the first intervention\n",
    "            None,                                     <- for the second intervention\n",
    "            [[subspace indices], [subspace indices]]\n",
    "        ]\n",
    "\n",
    "        Only setter (where do_intervention is called) needs this field.\n",
    "\n",
    "        *We assume base and source targetting the same subspace for now.\n",
    "        *We assume only a single space is targeted for now (although 2d list is provided).\n",
    "\n",
    "        Since we now support group-based intervention, the number of sources\n",
    "        should be equal to the total number of groups.\n",
    "        \"\"\"\n",
    "        # TODO: forgive me now, i will change this later.\n",
    "        activations_sources = source_representations\n",
    "        if sources is not None and not isinstance(sources, list):\n",
    "            sources = [sources]\n",
    "        \n",
    "        self._cleanup_states()\n",
    "\n",
    "        # if no source input or intervention, we return base\n",
    "        if sources is None and activations_sources is None \\\n",
    "            and unit_locations is None and len(self.interventions) == 0:\n",
    "            return self.model(**base), None\n",
    "        \n",
    "        # broadcast\n",
    "        unit_locations = self._broadcast_unit_locations(get_batch_size(base), unit_locations)\n",
    "        \n",
    "        sources = [None]*len(self._intervention_group) if sources is None else sources\n",
    "        sources = self._broadcast_sources(sources)\n",
    "        activations_sources = self._broadcast_source_representations(activations_sources)\n",
    "        subspaces = self._broadcast_subspaces(get_batch_size(base), subspaces)\n",
    "        \n",
    "        self._input_validation(\n",
    "            base,\n",
    "            sources,\n",
    "            unit_locations,\n",
    "            activations_sources,\n",
    "            subspaces,\n",
    "        )\n",
    "        \n",
    "        base_outputs = None\n",
    "        if output_original_output:\n",
    "            # returning un-intervened output with gradients\n",
    "            # False by default\n",
    "            base_outputs = self.model(**base)\n",
    "\n",
    "        try:\n",
    "            # intervene using hooks\n",
    "            if self.mode == \"parallel\":\n",
    "                set_handlers_to_remove = (\n",
    "                    self._wait_for_forward_with_parallel_intervention(\n",
    "                        sources,\n",
    "                        unit_locations,\n",
    "                        activations_sources,\n",
    "                        subspaces,\n",
    "                    )\n",
    "                )\n",
    "            elif self.mode == \"serial\":\n",
    "                set_handlers_to_remove = (\n",
    "                    self._wait_for_forward_with_serial_intervention(\n",
    "                        sources,\n",
    "                        unit_locations,\n",
    "                        activations_sources,\n",
    "                        subspaces,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # run intervened forward\n",
    "            model_kwargs = {}\n",
    "            if labels is not None: # for training\n",
    "                model_kwargs[\"labels\"] = labels\n",
    "            if 'use_cache' in self.model.config.to_dict(): # for transformer models\n",
    "                model_kwargs[\"use_cache\"] = use_cache\n",
    "\n",
    "            # ex : hook in register_forward_pre_hook called every time before forward() is invoked\n",
    "            # https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_forward_pre_hook\n",
    "            counterfactual_outputs = self.model(**base, **model_kwargs)\n",
    "\n",
    "            set_handlers_to_remove.remove()\n",
    "\n",
    "            self._output_validation()\n",
    "            \n",
    "            collected_activations = []\n",
    "            if self.return_collect_activations:\n",
    "                for key in self.sorted_keys:\n",
    "                    if isinstance(\n",
    "                        self.interventions[key][0],\n",
    "                        CollectIntervention\n",
    "                    ):\n",
    "                        collected_activations += self.activations[key]\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        finally:\n",
    "            self._cleanup_states(\n",
    "                skip_activation_gc = \\\n",
    "                    (sources is None and activations_sources is not None) or \\\n",
    "                    self.return_collect_activations\n",
    "            )\n",
    "        \n",
    "        if self.return_collect_activations:\n",
    "            if return_dict:\n",
    "                return IntervenableModelOutput(\n",
    "                    original_outputs=base_outputs,\n",
    "                    intervened_outputs=counterfactual_outputs,\n",
    "                    collected_activations=collected_activations\n",
    "                )\n",
    "            \n",
    "            return (base_outputs, collected_activations), counterfactual_outputs\n",
    "        \n",
    "        if return_dict:\n",
    "            return IntervenableModelOutput(\n",
    "                original_outputs=base_outputs,\n",
    "                intervened_outputs=counterfactual_outputs,\n",
    "                collected_activations=None\n",
    "            )\n",
    "\n",
    "        return base_outputs, counterfactual_outputs\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        base,\n",
    "        sources: Optional[List] = None,\n",
    "        unit_locations: Optional[Dict] = None,\n",
    "        source_representations: Optional[Dict] = None,\n",
    "        intervene_on_prompt: bool = False,\n",
    "        subspaces: Optional[List] = None,\n",
    "        output_original_output: Optional[bool] = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Intervenable generation function that serves a\n",
    "        wrapper to regular model generate calls.\n",
    "\n",
    "        Currently, we support basic interventions **in the\n",
    "        prompt only**. We will support generation interventions\n",
    "        in the next release.\n",
    "\n",
    "        TODO: Unroll sources and intervene in the generation step.\n",
    "\n",
    "        Parameters:\n",
    "        base:                The base example.\n",
    "        sources:             A list of source examples.\n",
    "        unit_locations:      The intervention locations of\n",
    "                             base.\n",
    "        activations_sources: A list of representations.\n",
    "        intervene_on_prompt: Whether only intervene on prompt.\n",
    "        **kwargs:            All other generation parameters.\n",
    "\n",
    "        Return:\n",
    "        base_output: the non-intervened output of the base\n",
    "        input.\n",
    "        counterfactual_outputs: the intervened output of the\n",
    "        base input.\n",
    "        \"\"\"\n",
    "        # TODO: forgive me now, i will change this later.\n",
    "        activations_sources = source_representations\n",
    "        if sources is not None and not isinstance(sources, list):\n",
    "            sources = [sources]\n",
    "            \n",
    "        self._cleanup_states()\n",
    "\n",
    "        self._intervene_on_prompt = intervene_on_prompt\n",
    "        self._is_generation = True\n",
    "        \n",
    "        if not intervene_on_prompt and unit_locations is None:\n",
    "            # that means, we intervene on every generated tokens!\n",
    "            unit_locations = {\"base\": 0}\n",
    "        \n",
    "        # broadcast\n",
    "        unit_locations = self._broadcast_unit_locations(get_batch_size(base), unit_locations)\n",
    "        sources = [None]*len(self._intervention_group) if sources is None else sources\n",
    "        sources = self._broadcast_sources(sources)\n",
    "        activations_sources = self._broadcast_source_representations(activations_sources)\n",
    "        subspaces = self._broadcast_subspaces(get_batch_size(base), subspaces)\n",
    "        \n",
    "        self._input_validation(\n",
    "            base,\n",
    "            sources,\n",
    "            unit_locations,\n",
    "            activations_sources,\n",
    "            subspaces,\n",
    "        )\n",
    "        \n",
    "        base_outputs = None\n",
    "        if output_original_output:\n",
    "            # returning un-intervened output\n",
    "            base_outputs = self.model.generate(**base, **kwargs)\n",
    "\n",
    "        set_handlers_to_remove = None\n",
    "        try:\n",
    "            # intervene\n",
    "            if self.mode == \"parallel\":\n",
    "                set_handlers_to_remove = (\n",
    "                    self._wait_for_forward_with_parallel_intervention(\n",
    "                        sources,\n",
    "                        unit_locations,\n",
    "                        activations_sources,\n",
    "                        subspaces,\n",
    "                    )\n",
    "                )\n",
    "            elif self.mode == \"serial\":\n",
    "                set_handlers_to_remove = (\n",
    "                    self._wait_for_forward_with_serial_intervention(\n",
    "                        sources,\n",
    "                        unit_locations,\n",
    "                        activations_sources,\n",
    "                        subspaces,\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "            # run intervened generate\n",
    "            counterfactual_outputs = self.model.generate(\n",
    "                **base, **kwargs\n",
    "            )\n",
    "            \n",
    "            collected_activations = []\n",
    "            if self.return_collect_activations:\n",
    "                for key in self.sorted_keys:\n",
    "                    if isinstance(\n",
    "                        self.interventions[key][0],\n",
    "                        CollectIntervention\n",
    "                    ):\n",
    "                        collected_activations += self.activations[key]\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        finally:\n",
    "            if set_handlers_to_remove is not None:\n",
    "                set_handlers_to_remove.remove()\n",
    "            self._is_generation = False\n",
    "            self._cleanup_states(\n",
    "                skip_activation_gc = \\\n",
    "                    (sources is None and activations_sources is not None) or \\\n",
    "                    self.return_collect_activations\n",
    "            )\n",
    "        \n",
    "        if self.return_collect_activations:\n",
    "            return (base_outputs, collected_activations), counterfactual_outputs\n",
    "        \n",
    "        return base_outputs, counterfactual_outputs\n",
    "\n",
    "    def _batch_process_unit_location(self, inputs):\n",
    "        \"\"\"\n",
    "        Convert original data batch according\n",
    "        to the intervenable settings.\n",
    "\n",
    "        The function respects inputs in the following\n",
    "        data format.\n",
    "\n",
    "\n",
    "        Each location list in the raw input as,\n",
    "\n",
    "        [[i, j, ...], [m, n, ...], ...] batched\n",
    "        where i, j are the unit index, the outter\n",
    "        list is for the batch\n",
    "\n",
    "\n",
    "        Possible fields in the input:\n",
    "\n",
    "        inputs[\"source_0->base.0.pos\"] -> batched\n",
    "        inputs[\"source_0->base.1.pos\"] -> batched\n",
    "        AND\n",
    "        inputs[\"source_0->source_1.0.pos\"] -> batched\n",
    "        inputs[\"source_0->source_1.1.pos\"] -> batched\n",
    "        ...\n",
    "\n",
    "        multiple source locations are included in case\n",
    "        there are multiple sources.\n",
    "\n",
    "        We also need to consider whether we are doing\n",
    "        parallel or serial interventions.\n",
    "\n",
    "        We also need to consider the granularity. In case\n",
    "        we are intervening h.pos, which is a specific location\n",
    "        in a specific head:\n",
    "\n",
    "        inputs[\"source_0->base.0.pos\"] -> batched\n",
    "        inputs[\"source_0->source_1.0.h\"] -> batched\n",
    "\n",
    "        inputs[\"source_0->base.0.pos\"] -> batched\n",
    "        inputs[\"source_0->source_1.0.pos\"] -> batched\n",
    "        \"\"\"\n",
    "        batched_location_dict = {}\n",
    "\n",
    "        _source_ind = []\n",
    "        for k, _ in inputs.items():\n",
    "            if \"->\" in k:\n",
    "                for sub_k in k.split(\"->\"):\n",
    "                    if \"source\" in sub_k:\n",
    "                        _source_ind += [int(sub_k.split(\"_\")[1])]\n",
    "        _max_source_ind = max(_source_ind)\n",
    "\n",
    "        # we assume source_0 -> source_1, ..., source_last -> base\n",
    "        # each pair uses an intervention\n",
    "\n",
    "        if self.mode == \"parallel\":\n",
    "            # all source into base at once but may engage different locations\n",
    "            _curr_source_ind = 0\n",
    "            _parallel_aggr_left = []\n",
    "            _parallel_aggr_right = []\n",
    "            for _, rep in self.representations.items():\n",
    "                _curr_source_ind_inc = _curr_source_ind + 1\n",
    "                _prefix = f\"source_{_curr_source_ind}->base\"\n",
    "                _prefix_left = f\"{_prefix}.0\"\n",
    "                _prefix_right = f\"{_prefix}.1\"\n",
    "                _sub_loc_aggr_left = []  # 3d\n",
    "                _sub_loc_aggr_right = []  # 3d\n",
    "                for sub_loc in rep.unit.split(\".\"):\n",
    "                    _sub_loc_aggr_left += [inputs[f\"{_prefix_left}.{sub_loc}\"]]\n",
    "                    _sub_loc_aggr_right += [inputs[f\"{_prefix_right}.{sub_loc}\"]]\n",
    "                if len(rep.unit.split(\".\")) == 1:\n",
    "                    _sub_loc_aggr_left = _sub_loc_aggr_left[0]\n",
    "                    _sub_loc_aggr_right = _sub_loc_aggr_right[0]\n",
    "                _parallel_aggr_left += [_sub_loc_aggr_left]  # 3D or 4D\n",
    "                _parallel_aggr_right += [_sub_loc_aggr_right]  # 3D or 4D\n",
    "                _curr_source_ind += 1\n",
    "\n",
    "            batched_location_dict[\"sources->base\"] = (\n",
    "                _parallel_aggr_left,\n",
    "                _parallel_aggr_right,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # source into another source and finally to the base engaging different locations\n",
    "            _curr_source_ind = 0\n",
    "            for _, rep in self.representations.items():\n",
    "                _curr_source_ind_inc = _curr_source_ind + 1\n",
    "                _prefix = (\n",
    "                    f\"source_{_curr_source_ind}->base\"\n",
    "                    if _curr_source_ind + 1 == len(self.representations)\n",
    "                    else f\"source_{_curr_source_ind}->source{_curr_source_ind_inc}\"\n",
    "                )\n",
    "                _prefix_left = f\"{_prefix}.0\"\n",
    "                _prefix_right = f\"{_prefix}.1\"\n",
    "                _sub_loc_aggr_left = []  # 3d\n",
    "                _sub_loc_aggr_right = []  # 3d\n",
    "                for sub_loc in rep.unit.split(\".\"):\n",
    "                    _sub_loc_aggr_left += [inputs[f\"{_prefix_left}.{sub_loc}\"]]\n",
    "                    _sub_loc_aggr_right += [inputs[f\"{_prefix_right}.{sub_loc}\"]]\n",
    "                if len(rep.unit.split(\".\")) == 1:\n",
    "                    _sub_loc_aggr_left = _sub_loc_aggr_left[0]\n",
    "                    _sub_loc_aggr_right = _sub_loc_aggr_right[0]\n",
    "                _curr_source_ind += 1\n",
    "                batched_location_dict[_prefix] = (\n",
    "                    [_sub_loc_aggr_left],  # 3D or 4D\n",
    "                    [_sub_loc_aggr_right],  # 3D or 4D\n",
    "                )\n",
    "\n",
    "        return batched_location_dict\n",
    "    \n",
    "    def train(self, mode=True):\n",
    "        self.model.train(mode=mode)\n",
    "    \n",
    "    def eval(self):\n",
    "        self.model.eval()\n",
    "\n",
    "    def train_alignment(\n",
    "        self,\n",
    "        train_dataloader,\n",
    "        compute_loss,\n",
    "        compute_metrics,\n",
    "        inputs_collator,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The method find alignment.\n",
    "\n",
    "        a.k.a. training the intervention\n",
    "\n",
    "        Notes:\n",
    "        1) we use Adam, and linear lr scheduling.\n",
    "        2) you can pass in lr or using default 1e-3\n",
    "        \"\"\"\n",
    "        # preprocess basic kwargs\n",
    "        lr = kwargs[\"lr\"] if \"lr\" in kwargs else 1e-3\n",
    "        epochs = kwargs[\"epochs\"] if \"epochs\" in kwargs else 10\n",
    "        warm_up_steps = kwargs[\"warm_up_steps\"] if \"warm_up_steps\" in kwargs else 0.1\n",
    "        gradient_accumulation_steps = (\n",
    "            kwargs[\"gradient_accumulation_steps\"]\n",
    "            if \"gradient_accumulation_steps\" in kwargs\n",
    "            else 1\n",
    "        )\n",
    "\n",
    "        # some deeper kwargs\n",
    "        t_total = int(len(train_dataloader) * epochs)\n",
    "        warm_up_steps = 0.1 * t_total\n",
    "        target_total_step = len(train_dataloader) * epochs\n",
    "        optimizer_params = [{\"params\": self.get_trainable_parameters()}]\n",
    "        optimizer = (\n",
    "            kwargs[\"optimizer\"]\n",
    "            if \"optimizer\" in kwargs\n",
    "            else optim.Adam(optimizer_params, lr=lr)\n",
    "        )\n",
    "        scheduler = (\n",
    "            kwargs[\"scheduler\"]\n",
    "            if \"scheduler\" in kwargs\n",
    "            else get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=warm_up_steps, num_training_steps=t_total\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # in case we need additional temp scheduling\n",
    "        temperature_start = 50.0\n",
    "        temperature_end = 0.1\n",
    "        temperature_schedule = (\n",
    "            torch.linspace(temperature_start, temperature_end, target_total_step)\n",
    "            .to(torch.bfloat16)\n",
    "            .to(self.get_device())\n",
    "        )\n",
    "\n",
    "        # train main loop\n",
    "        remove_forward_hooks(self.model)\n",
    "        self.model.eval()  # train enables drop-off but no grads\n",
    "        epoch_iterator = trange(0, int(epochs), desc=\"Epoch\")\n",
    "        total_step = 0\n",
    "        for epoch in epoch_iterator:\n",
    "            for step, inputs in enumerate(train_dataloader):\n",
    "                if inputs_collator is not None:\n",
    "                    inputs = inputs_collator(inputs)\n",
    "                b_s = inputs[\"input_ids\"].shape[0]\n",
    "                unit_location_dict = self._batch_process_unit_location(inputs)\n",
    "                _, counterfactual_outputs = self(\n",
    "                    {\"input_ids\": inputs[\"input_ids\"]},\n",
    "                    [{\"input_ids\": inputs[\"source_input_ids\"]}],\n",
    "                    unit_location_dict,\n",
    "                    subspaces=inputs[\"subspaces\"] if \"subspaces\" in inputs else None,\n",
    "                )\n",
    "                eval_metrics = compute_metrics(\n",
    "                    [counterfactual_outputs.logits], [inputs[\"labels\"]]\n",
    "                )\n",
    "\n",
    "                # loss and backprop\n",
    "                loss = compute_loss(counterfactual_outputs.logits, inputs[\"labels\"])\n",
    "                loss_str = round(loss.item(), 2)\n",
    "                epoch_iterator.set_postfix({\"loss\": loss_str, \"acc\": eval_metrics})\n",
    "\n",
    "                if gradient_accumulation_steps > 1:\n",
    "                    loss = loss / gradient_accumulation_steps\n",
    "                loss.backward()\n",
    "                if total_step % gradient_accumulation_steps == 0:\n",
    "                    if not (gradient_accumulation_steps > 1 and total_step == 0):\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "                        self.set_zero_grad()\n",
    "                        self.set_temperature(temperature_schedule[total_step])\n",
    "                total_step += 1\n",
    "\n",
    "    def eval_alignment(\n",
    "        self,\n",
    "        eval_dataloader,\n",
    "        compute_metrics,\n",
    "        inputs_collator,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The method evaluate alignment.\n",
    "        \"\"\"\n",
    "\n",
    "        all_metrics = []\n",
    "        all_num_examples = []\n",
    "        torch.cuda.empty_cache()\n",
    "        with torch.no_grad():\n",
    "            for inputs in tqdm(eval_dataloader, desc=\"Evaluating\", leave=False):\n",
    "                if inputs_collator is not None:\n",
    "                    inputs = inputs_collator(inputs)\n",
    "                b_s = inputs[\"input_ids\"].shape[0]\n",
    "                unit_location_dict = self._batch_process_unit_location(\n",
    "                    inputs,\n",
    "                )\n",
    "                _, counterfactual_outputs = self(\n",
    "                    {\"input_ids\": inputs[\"input_ids\"]},\n",
    "                    [{\"input_ids\": inputs[\"source_input_ids\"]}],\n",
    "                    unit_location_dict,\n",
    "                    subspaces=inputs[\"subspaces\"] if \"subspaces\" in inputs else None,\n",
    "                )\n",
    "                eval_metrics = compute_metrics(\n",
    "                    [counterfactual_outputs.logits], [inputs[\"labels\"]]\n",
    "                )\n",
    "                all_metrics += [eval_metrics]\n",
    "                all_num_examples += [b_s]\n",
    "        result = weighted_average(all_metrics, all_num_examples)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "def build_intervenable_model(config, model, **kwargs):\n",
    "    \"\"\"\n",
    "    Factory design pattern for different types of intervenable models.\n",
    "    \"\"\"\n",
    "    if isinstance(model, nnsight.LanguageModel):\n",
    "        return IntervenableNdifModel(config, model, **kwargs)\n",
    "    else:\n",
    "        return IntervenableModel(config, model, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "import inspect\n",
    "import itertools\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CausalModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        variables,\n",
    "        values,\n",
    "        parents,\n",
    "        functions,\n",
    "        timesteps=None,\n",
    "        equiv_classes=None,\n",
    "        pos={},\n",
    "    ):\n",
    "        self.variables = variables\n",
    "        self.variables.sort()\n",
    "        self.values = values\n",
    "        self.parents = parents\n",
    "        self.children = {var: [] for var in variables}\n",
    "        for variable in variables:\n",
    "            assert variable in self.parents\n",
    "            for parent in self.parents[variable]:\n",
    "                self.children[parent].append(variable)\n",
    "        self.functions = functions\n",
    "        self.start_variables = []\n",
    "        self.timesteps = timesteps\n",
    "        for variable in self.variables:\n",
    "            assert variable in self.values\n",
    "            assert variable in self.children\n",
    "            assert variable in self.functions\n",
    "            if timesteps is not None:\n",
    "                assert variable in timesteps\n",
    "            for variable2 in copy.copy(self.variables):\n",
    "                if variable2 in self.parents[variable]:\n",
    "                    assert variable in self.children[variable2]\n",
    "                    if timesteps is not None:\n",
    "                        assert timesteps[variable2] < timesteps[variable]\n",
    "                if variable2 in self.children[variable]:\n",
    "                    assert variable in parents[variable2]\n",
    "                    if timesteps is not None:\n",
    "                        assert timesteps[variable2] > timesteps[variable]\n",
    "            if len(self.parents) == 0:\n",
    "                self.start_variables.append(variable)\n",
    "\n",
    "        # leaf nodes\n",
    "        self.inputs = [var for var in self.variables if len(parents[var]) == 0]\n",
    "        # get root (output)\n",
    "        self.outputs = copy.deepcopy(variables)\n",
    "        for child in variables:\n",
    "            for parent in parents[child]:\n",
    "                if parent in self.outputs:\n",
    "                    self.outputs.remove(parent)\n",
    "\n",
    "        if self.timesteps is not None:\n",
    "            self.timesteps = timesteps\n",
    "        else:\n",
    "            self.timesteps, self.end_time = self.generate_timesteps()\n",
    "            for output in self.outputs:\n",
    "                self.timesteps[output] = self.end_time\n",
    "        self.variables.sort(key=lambda x: self.timesteps[x])\n",
    "        # tests forward_run\n",
    "        #self.run_forward()\n",
    "        self.pos = pos\n",
    "        width = {_: 0 for _ in range(len(self.variables))}\n",
    "        if self.pos == None:\n",
    "            self.pos = dict()\n",
    "        for var in self.variables:\n",
    "            if var not in pos:\n",
    "                pos[var] = (width[self.timesteps[var]], self.timesteps[var])\n",
    "                width[self.timesteps[var]] += 1\n",
    "\n",
    "        if equiv_classes is not None:\n",
    "            self.equiv_classes = equiv_classes\n",
    "        else:\n",
    "            self.equiv_classes = {}\n",
    "    \n",
    "\n",
    "    def generate_equiv_classes(self):\n",
    "        for var in self.variables:\n",
    "            if var in self.inputs or var in self.equiv_classes:\n",
    "                continue\n",
    "            self.equiv_classes[var] = {val: [] for val in self.values[var]}\n",
    "            for parent_values in itertools.product(\n",
    "                *[self.values[par] for par in self.parents[var]]\n",
    "            ):\n",
    "                value = self.functions[var](*parent_values)\n",
    "                self.equiv_classes[var][value].append(\n",
    "                    {par: parent_values[i] for i, par in enumerate(self.parents[var])}\n",
    "                )\n",
    "                \n",
    "\n",
    "    def generate_timesteps(self):\n",
    "        timesteps = {input: 0 for input in self.inputs}\n",
    "        step = 1\n",
    "        change = True\n",
    "        while change:\n",
    "            change = False\n",
    "            copytimesteps = copy.deepcopy(timesteps)\n",
    "            for parent in timesteps:\n",
    "                if timesteps[parent] == step - 1:\n",
    "                    for child in self.children[parent]:\n",
    "                        copytimesteps[child] = step\n",
    "                        change = True\n",
    "            timesteps = copytimesteps\n",
    "            step += 1\n",
    "        for var in self.variables:\n",
    "            assert var in timesteps\n",
    "        # return all timesteps and timestep of root\n",
    "        return timesteps, step - 2\n",
    "\n",
    "    \n",
    "    def marginalize(self, target):\n",
    "        pass\n",
    "\n",
    "                \n",
    "    def print_structure(self, pos=None, font=12, node_size=1000):\n",
    "        G = nx.DiGraph()\n",
    "        G.add_edges_from(\n",
    "            [\n",
    "                (parent, child)\n",
    "                for child in self.variables\n",
    "                for parent in self.parents[child]\n",
    "            ]\n",
    "        )\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        nx.draw_networkx(G, with_labels=True, node_color=\"green\", pos=self.pos, font_size=font, node_size=node_size)\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    def find_live_paths(self, intervention):\n",
    "        actual_setting = self.run_forward(intervention)\n",
    "        paths = {1: [[variable] for variable in self.variables]}\n",
    "        step = 2\n",
    "        while True:\n",
    "            paths[step] = []\n",
    "            for path in paths[step - 1]:\n",
    "                for child in self.children[path[-1]]:\n",
    "                    actual_cause = False\n",
    "                    for value in self.values[path[-1]]:\n",
    "                        newintervention = copy.deepcopy(intervention)\n",
    "                        newintervention[path[-1]] = value\n",
    "                        counterfactual_setting = self.run_forward(newintervention)\n",
    "                        if counterfactual_setting[child] != actual_setting[child]:\n",
    "                            actual_cause = True\n",
    "                    if actual_cause:\n",
    "                        paths[step].append(copy.deepcopy(path) + [child])\n",
    "            if len(paths[step]) == 0:\n",
    "                break\n",
    "            step += 1\n",
    "        del paths[1]\n",
    "        return paths\n",
    "\n",
    "    \n",
    "    def print_setting(self, total_setting, font=12, node_size=1000):\n",
    "        relabeler = {\n",
    "            var: var + \": \" + str(total_setting[var]) for var in self.variables\n",
    "        }\n",
    "        G = nx.DiGraph()\n",
    "        G.add_edges_from(\n",
    "            [\n",
    "                (parent, child)\n",
    "                for child in self.variables\n",
    "                for parent in self.parents[child]\n",
    "            ]\n",
    "        )\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        G = nx.relabel_nodes(G, relabeler)\n",
    "        newpos = dict()\n",
    "        if self.pos is not None:\n",
    "            for var in self.pos:\n",
    "                newpos[relabeler[var]] = self.pos[var]\n",
    "        nx.draw_networkx(G, with_labels=True, node_color=\"green\", pos=newpos, font_size=font, node_size=node_size)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def run_forward(self, intervention=None):\n",
    "        total_setting = defaultdict(None)\n",
    "        length = len(list(total_setting.keys()))\n",
    "        step = 0\n",
    "        while length != len(self.variables):\n",
    "            for variable in self.variables:\n",
    "                for variable2 in self.parents[variable]:\n",
    "                    if variable2 not in total_setting:\n",
    "                        continue\n",
    "                if intervention is not None and variable in intervention:\n",
    "                    total_setting[variable] = intervention[variable]\n",
    "                else:\n",
    "                    total_setting[variable] = self.functions[variable](*[total_setting[parent] for parent in self.parents[variable]])\n",
    "            length = len(list(total_setting.keys()))\n",
    "        return total_setting\n",
    "\n",
    "\n",
    "    # example :\n",
    "    # base = {\"W\": reps[0], \"X\": reps[0], \"Y\": reps[1], \"Z\": reps[3]}\n",
    "    # source = {\"W\": reps[0], \"X\": reps[1], \"Y\": reps[2], \"Z\": reps[2]}\n",
    "    # setting = equality_model.run_interchange(base, {\"WX\": source})\n",
    "    def run_interchange(self, input, source_interventions):\n",
    "        interchange_intervention = copy.deepcopy(input)\n",
    "        for var in source_interventions:\n",
    "            setting = self.run_forward(source_interventions[var])\n",
    "            interchange_intervention[var] = setting[var]\n",
    "        return self.run_forward(interchange_intervention)\n",
    "\n",
    "\n",
    "    def add_variable(\n",
    "        self, variable, values, parents, children, function, timestep=None\n",
    "    ):\n",
    "        if timestep is not None:\n",
    "            assert self.timesteps is not None\n",
    "            self.timesteps[variable] = timestep\n",
    "        for parent in parents:\n",
    "            assert parent in self.variables\n",
    "        for child in children:\n",
    "            assert child in self.variables\n",
    "        self.parents[variable] = parents\n",
    "        self.children[variable] = children\n",
    "        self.values[variable] = values\n",
    "        self.functions[variable] = function\n",
    "\n",
    "\n",
    "    def sample_intervention(self, mandatory=None):\n",
    "        intervention = {}\n",
    "        while len(intervention.keys()) == 0:\n",
    "            # only consider non leaf and non root nodes\n",
    "            for var in self.variables:\n",
    "                if var in self.inputs or var in self.outputs:\n",
    "                    continue\n",
    "                if random.choice([0, 1]) == 0:\n",
    "                    # values gives all possible values of a variable\n",
    "                    intervention[var] = random.choice(self.values[var])\n",
    "        return intervention\n",
    "\n",
    "\n",
    "    def sample_input(self, mandatory=None):\n",
    "        input = {var: random.sample(self.values[var], 1)[0] for var in self.inputs}\n",
    "        total = self.run_forward(intervention=input)\n",
    "        while mandatory is not None and not mandatory(total):\n",
    "            input = {var: random.sample(self.values[var], 1)[0] for var in self.inputs}\n",
    "            total = self.run_forward(intervention=input)\n",
    "        return input\n",
    "\n",
    "\n",
    "    def sample_input_tree_balanced(self, output_var=None, output_var_value=None):\n",
    "        assert output_var is not None or len(self.outputs) == 1\n",
    "        \n",
    "        self.generate_equiv_classes()\n",
    "\n",
    "        if output_var is None:\n",
    "            output_var = self.outputs[0]\n",
    "        if output_var_value is None:\n",
    "            output_var_value = random.choice(self.values[output_var])\n",
    "\n",
    "        def create_input(var, value, input={}):\n",
    "            parent_values = random.choice(self.equiv_classes[var][value])\n",
    "            for parent in parent_values:\n",
    "                if parent in self.inputs:\n",
    "                    input[parent] = parent_values[parent]\n",
    "                else:\n",
    "                    create_input(parent, parent_values[parent], input)\n",
    "            return input\n",
    "\n",
    "        input_setting = create_input(output_var, output_var_value)\n",
    "        for input_var in self.inputs:\n",
    "            if input_var not in input_setting:\n",
    "                input_setting[input_var] = random.choice(self.values[input_var])\n",
    "        return input_setting\n",
    "\n",
    "\n",
    "    def get_path_maxlen_filter(self, lengths):\n",
    "        def check_path(total_setting):\n",
    "            input = {var: total_setting[var] for var in self.inputs}\n",
    "            paths = self.find_live_paths(input)\n",
    "            m = max([l for l in paths.keys() if len(paths[l]) != 0])\n",
    "            if m in lengths:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "        return check_path\n",
    "\n",
    "    def get_partial_filter(self, partial_setting):\n",
    "        def compare(total_setting):\n",
    "            for var in partial_setting:\n",
    "                if total_setting[var] != partial_setting[var]:\n",
    "                    return False\n",
    "            return True\n",
    "\n",
    "        return compare\n",
    "\n",
    "    \n",
    "    def get_specific_path_filter(self, start, end):\n",
    "        def check_path(total_setting):\n",
    "            input = {var: total_setting[var] for var in self.inputs}\n",
    "            paths = self.find_live_paths(input)\n",
    "            for k in paths:\n",
    "                for path in paths[k]:\n",
    "                    if path[0] == start and path[-1] == end:\n",
    "                        return True\n",
    "            return False\n",
    "\n",
    "        return check_path\n",
    "\n",
    "    \n",
    "    def input_to_tensor(self, setting):\n",
    "        result = []\n",
    "        for input in self.inputs:\n",
    "            temp = torch.tensor(setting[input]).float()\n",
    "            if len(temp.size()) == 0:\n",
    "                temp = torch.reshape(temp, (1,))\n",
    "            result.append(temp)\n",
    "        return torch.cat(result)\n",
    "\n",
    "    \n",
    "    def output_to_tensor(self, setting):\n",
    "        result = []\n",
    "        for output in self.outputs:\n",
    "            temp = torch.tensor(float(setting[output]))\n",
    "            if len(temp.size()) == 0:\n",
    "                temp = torch.reshape(temp, (1,))\n",
    "            result.append(temp)\n",
    "        return torch.cat(result)\n",
    "\n",
    "        \n",
    "    def generate_factual_dataset(\n",
    "        self,\n",
    "        size,\n",
    "        sampler=None,\n",
    "        filter=None,\n",
    "        device=\"cpu\",\n",
    "        input_function=None,\n",
    "        output_function=None,\n",
    "        return_tensors=True,\n",
    "    ):\n",
    "        if sampler is None:\n",
    "            sampler = self.sample_input\n",
    "        \n",
    "        if input_function is None:\n",
    "            input_function = self.input_to_tensor\n",
    "        if output_function is None:\n",
    "            output_function = self.output_to_tensor\n",
    "\n",
    "        examples = []\n",
    "        while len(examples) < size:\n",
    "            example = dict()\n",
    "            input = sampler()\n",
    "            if filter is None or filter(input):\n",
    "                output = self.run_forward(input)\n",
    "                if return_tensors:\n",
    "                    example['input_ids'] = input_function(input).to(device)\n",
    "                    example['labels'] = output_function(output).to(device)\n",
    "                else:\n",
    "                    example['input_ids'] = input\n",
    "                    example['labels'] = output\n",
    "                examples.append(example)\n",
    "\n",
    "        return examples\n",
    "\n",
    "    \n",
    "    def generate_counterfactual_dataset(\n",
    "        self,\n",
    "        size,\n",
    "        intervention_id,\n",
    "        batch_size,\n",
    "        sampler=None,\n",
    "        intervention_sampler=None,\n",
    "        filter=None,\n",
    "        device=\"cpu\",\n",
    "        input_function=None,\n",
    "        output_function=None,\n",
    "        return_tensors=True,\n",
    "    ):\n",
    "        if input_function is None:\n",
    "            input_function = self.input_to_tensor\n",
    "        if output_function is None:\n",
    "            output_function = self.output_to_tensor\n",
    "\n",
    "        maxlength = len(\n",
    "            [\n",
    "                var\n",
    "                for var in self.variables\n",
    "                if var not in self.inputs and var not in self.outputs\n",
    "            ]\n",
    "        )\n",
    "        if sampler is None:\n",
    "            sampler = self.sample_input\n",
    "        if intervention_sampler is None:\n",
    "            intervention_sampler = self.sample_intervention\n",
    "\n",
    "        examples = []\n",
    "        while len(examples) < size:\n",
    "            intervention = intervention_sampler()\n",
    "            if filter is None or filter(intervention):\n",
    "                for _ in range(batch_size):\n",
    "                    example = dict()\n",
    "                    base = sampler()\n",
    "                    sources = []\n",
    "                    source_dic = {}\n",
    "                    for var in self.variables:\n",
    "                        if var not in intervention:\n",
    "                            continue\n",
    "                        # sample input to match sampled intervention value\n",
    "                        source = sampler(output_var=var, output_var_value=intervention[var])\n",
    "                        if return_tensors:\n",
    "                            sources.append(self.input_to_tensor(source))\n",
    "                        else:\n",
    "                            sources.append(source)\n",
    "                        source_dic[var] = source\n",
    "\n",
    "                    for _ in range(maxlength - len(sources)):\n",
    "                        if return_tensors:\n",
    "                            sources.append(torch.zeros(self.input_to_tensor(base).shape))\n",
    "                        else:\n",
    "                            sources.append({})\n",
    "\n",
    "                    if return_tensors:\n",
    "                        example[\"labels\"] = self.output_to_tensor(\n",
    "                            self.run_interchange(base, source_dic)\n",
    "                        ).to(device)\n",
    "                        example[\"base_labels\"] = self.output_to_tensor(\n",
    "                            self.run_forward(base)\n",
    "                        ).to(device)\n",
    "                        example[\"input_ids\"] = self.input_to_tensor(base).to(device)\n",
    "                        example[\"source_input_ids\"] = torch.stack(sources).to(device)\n",
    "                        example[\"intervention_id\"] = torch.tensor(\n",
    "                            [intervention_id(intervention)]\n",
    "                        ).to(device)\n",
    "                    else:\n",
    "                        example['labels'] = self.run_interchange(base, source_dic)\n",
    "                        example['base_labels'] = self.run_forward(base)\n",
    "                        example['input_ids'] = base\n",
    "                        example['source_input_ids'] = sources\n",
    "                        example['intervention_id'] = [intervention_id(intervention)]\n",
    "\n",
    "                    examples.append(example)\n",
    "        return examples\n",
    "\n",
    "\n",
    "def simple_example():\n",
    "    variables = [\"A\", \"B\", \"C\"]\n",
    "    values = {variable: [True, False] for variable in variables}\n",
    "    parents = {\"A\": [], \"B\": [], \"C\": [\"A\", \"B\"]}\n",
    "\n",
    "    def A():\n",
    "        return True\n",
    "\n",
    "    def B():\n",
    "        return False\n",
    "\n",
    "    def C(a, b):\n",
    "        return a and b\n",
    "\n",
    "    functions = {\"A\": A, \"B\": B, \"C\": C}\n",
    "    model = CausalModel(variables, values, parents, functions)\n",
    "    model.print_structure()\n",
    "    print(\"No intervention:\\n\", model.run_forward(), \"\\n\")\n",
    "    model.print_setting(model.run_forward())\n",
    "    print(\n",
    "        \"Intervention setting A and B to TRUE:\\n\",\n",
    "        model.run_forward({\"A\": True, \"B\": True}),\n",
    "    )\n",
    "    print(\"Timesteps:\", model.timesteps)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #simple_example()\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand Crafting an MLP to Solve Hierarchical Equality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train a network to solve the hierarchical equality task, first consider an analytical solution where we define a neural network to have weights that are handcrafted to solve the task by implementing the algorithm $\\mathcal{A}$. The network is a two layer feedforward neural network that uses the ReLU function to compute the absolute difference between two vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MLPConfig(\n",
    "    h_dim=embedding_dim * 4,\n",
    "    activation_function=\"relu\",\n",
    "    n_layer=2,\n",
    "    num_classes=2,\n",
    "    pdrop=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "config, tokenizer, handcrafted = create_mlp_classifier(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer of our handcrafted model computes:\n",
    "\n",
    "$ReLU(W_1[\\mathbf{a}, \\mathbf{b}, \\mathbf{c}, \\mathbf{d}]) = [max(\\mathbf{a}-\\mathbf{b}, 0), max(\\mathbf{b}-\\mathbf{a}, 0), max(\\mathbf{c}-\\mathbf{d}, 0), max(\\mathbf{d}-\\mathbf{c}, 0)]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = [\n",
    "    [1, 0, -1, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 0, -1, 0, 0, 0, 0],\n",
    "    [-1, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, -1, 0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 1, 0, -1, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0, -1],\n",
    "    [0, 0, 0, 0, -1, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 0, -1, 0, 1],\n",
    "]\n",
    "handcrafted.mlp.h[0].ff1.weight = torch.nn.Parameter(torch.FloatTensor(W1))\n",
    "handcrafted.mlp.h[0].ff1.bias = torch.nn.Parameter(torch.FloatTensor([0, 0, 0, 0, 0, 0, 0, 0]))\n",
    "\n",
    "W2 = [\n",
    "    [1, -1, 0, 1, 0, 0, 0, 0],\n",
    "    [1, -1, 0, 1, 0, 0, 0, 0],\n",
    "    [1, -1, 0, 1, 0, 0, 0, 0],\n",
    "    [1, -1, 0, 1, 0, 0, 0, 0],\n",
    "    [-1, 1, 1, 0, 0, 0, 0, 0],\n",
    "    [-1, 1, 1, 0, 0, 0, 0, 0],\n",
    "    [-1, 1, 1, 0, 0, 0, 0, 0],\n",
    "    [-1, 1, 1, 0, 0, 0, 0, 0],\n",
    "]\n",
    "handcrafted.mlp.h[1].ff1.weight = torch.nn.Parameter(\n",
    "    torch.FloatTensor(W2).transpose(0, 1)\n",
    ")\n",
    "handcrafted.mlp.h[1].ff1.bias = torch.nn.Parameter(\n",
    "    torch.FloatTensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
    ")\n",
    "\n",
    "W3 = [[1, 0], [1, 0], [-0.999999, 0], [-0.999999, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
    "handcrafted.score.weight = torch.nn.Parameter(torch.FloatTensor(W3).transpose(0, 1))\n",
    "handcrafted.score.bias = torch.nn.Parameter(torch.FloatTensor([0, 0.00000000000001]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second layer of our handcrafted model computes:\n",
    "\n",
    "$ReLU(W_2ReLU(W_1[\\mathbf{a}, \\mathbf{b}, \\mathbf{c}, \\mathbf{d}])) = [|\\mathbf{a}-\\mathbf{b}| - |\\mathbf{c}-\\mathbf{d}|, |\\mathbf{c}-\\mathbf{d}|-|\\mathbf{a}-\\mathbf{b}|, |\\mathbf{a}-\\mathbf{b}|, |\\mathbf{c}-\\mathbf{d}|,0,0,0,0]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third layer of our handcrafted model computes the logits:\n",
    "\n",
    "$W_3 ReLU(W_2ReLU(W_1[\\mathbf{a}, \\mathbf{b}, \\mathbf{c}, \\mathbf{d}])) = [||\\mathbf{a}-\\mathbf{b}| - |\\mathbf{c}-\\mathbf{d}|| -0.999999|\\mathbf{a}-\\mathbf{b}|-0.999999|\\mathbf{c}-\\mathbf{d}|, 0]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal abstraction\n",
    "\n",
    "The theory of **causal abstraction** describes the conditions that must hold for the high-level tree structured algorithm to be a simplified and faithful description of the neural network. To perform causal abstraction analysis, we need to align high-level variables in our hypothesized algorithm $\\mathcal{A}$ with sets of low-level variables in the low-level neural network $\\mathcal{N}$. \n",
    "\n",
    "In essence: $\\mathcal{A}$ is a causal abstraction of a $\\mathcal{N}$ if and only if $\\mathcal{A}$ and $\\mathcal{N}$ provides the same output for all interchange interventions that target aligned variables.\n",
    "\n",
    "For our handcrafted network, **we align the first four neurons in the first feed-forward layer with the high-level variable 'WX' and align the other four neurons in that layer with 'YZ'**. Below, we create an IntervenableConfig that allows us to taget the first four and last four neurons of the first layer for an interchange intervention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = IntervenableConfig(\n",
    "    model_type=type(handcrafted),\n",
    "    representations=[\n",
    "        RepresentationConfig(\n",
    "            0,  # layer\n",
    "            \"block_output\",  # component\n",
    "            subspace_partition=[[0, 4], [4, 8]],\n",
    "        ),\n",
    "        RepresentationConfig(\n",
    "            0,  # layer\n",
    "            \"block_output\",  # component\n",
    "            subspace_partition=[[0, 4], [4, 8]],\n",
    "        ),\n",
    "    ],\n",
    "    intervention_types=VanillaIntervention,\n",
    ")\n",
    "handcrafted = IntervenableModel(config, handcrafted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(0, ['layer.0.comp.block_output.unit.pos.nunit.1#0']),\n",
       "             (1, ['layer.0.comp.block_output.unit.pos.nunit.1#1'])])"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handcrafted._intervention_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a counterfactual equality dataset that includes interchange intervention examples. We first define a function that creates an id for the three possible high-level interventions, namely targetting 'WX', targetting 'YZ', and targetting them both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map intervention variable to some id\n",
    "def intervention_id(intervention):\n",
    "    if \"WX\" in intervention and \"YZ\" in intervention:\n",
    "        return 2\n",
    "    if \"WX\" in intervention:\n",
    "        return 0\n",
    "    if \"YZ\" in intervention:\n",
    "        return 1\n",
    "\n",
    "\n",
    "data_size = 160\n",
    "batch_size = 16\n",
    "# batch_size needed since dataset batched together in terms of intervention id\n",
    "dataset = equality_model.generate_counterfactual_dataset(\n",
    "    data_size,\n",
    "    intervention_id,\n",
    "    batch_size,\n",
    "    #device=\"cuda:0\",\n",
    "    device='cpu',\n",
    "    sampler=equality_model.sample_input_tree_balanced,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has the following components:\n",
    "\n",
    "* `input_ids`: a regular set of train examples\n",
    "* `base_labels`: a regular set of train labels\n",
    "* `source_input_ids`: sets additional training inputs sets **(here, two sets)** for interchange interventions\n",
    "* `labels`: a list of labels if interchange interventions are performed with 'source_input_ids'\n",
    "* `intervention_id`: a list of intervention sites (here, all `0` corresponding to our key for \"V1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1600, -0.1900,  0.1600, -0.1900,  0.6600,  0.8300,  0.0000,  0.1700])\n",
      "tensor([[ 0.6600,  0.8300,  0.5800, -0.9500, -0.8500, -0.3600,  0.6600,  0.8300],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "tensor([0.])\n",
      "tensor([1.])\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"input_ids\"])\n",
    "print(dataset[0][\"source_input_ids\"])\n",
    "print(dataset[0][\"base_labels\"])\n",
    "print(dataset[0][\"labels\"])\n",
    "print(dataset[0][\"intervention_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model on this dataset, we loop through batches and peform interchange interventions based on the intervention_id. \n",
    "* When the id is 0, the first four neurons in the first layer are targetted ('WX' is targetted at the high-level)\n",
    "* When the id is 1, the last four neurons in the first layer are targetted ('YZ' is targetted at the high-level)\n",
    "* When the id is 2, all of the neurons in the first layer are targetted ('WX' and 'YZ' are both targetted at the high-level) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handcrafted.to(\"cuda:0\")\n",
    "#for parameter in handcrafted.get_trainable_parameters():\n",
    "    #parameter.to(\"cuda:0\")\n",
    "preds = []\n",
    "\n",
    "# VanillaIntervention\n",
    "# Intervenable model forward :\n",
    "# base,\n",
    "# sources: Optional[List] = None, \n",
    "# unit_locations: Optional[Dict] = None,\n",
    "# source_representations: Optional[Dict] = None,\n",
    "# subspaces: Optional[List] = None,\n",
    "# labels: Optional[torch.LongTensor] = None,\n",
    "# output_original_output: Optional[bool] = False,\n",
    "# return_dict: Optional[bool] = None,\n",
    "# use_cache: Optional[bool] = True,\n",
    "\n",
    "for batch in DataLoader(dataset, batch_size):\n",
    "    print('intervention_id')\n",
    "    print(batch[\"intervention_id\"][0])\n",
    "    print('')\n",
    "    batch[\"input_ids\"] = batch[\"input_ids\"].unsqueeze(1)\n",
    "    batch[\"source_input_ids\"] = batch[\"source_input_ids\"].unsqueeze(2)\n",
    "    if batch[\"intervention_id\"][0] == 2:  # Intervention on both high-level variables\n",
    "        _, counterfactual_outputs = handcrafted(\n",
    "            {\"inputs_embeds\": batch[\"input_ids\"]},  # base\n",
    "            [\n",
    "                {\"inputs_embeds\": batch[\"source_input_ids\"][:, 0]},  # source 1 \n",
    "                {\"inputs_embeds\": batch[\"source_input_ids\"][:, 1]},  # source 2\n",
    "            ],\n",
    "            {\n",
    "                \"sources->base\": (  # unit locations\n",
    "                    [[[0]] * batch_size, [[0]] * batch_size],  # unit_locations_sources\n",
    "                    [[[0]] * batch_size, [[0]] * batch_size],  # unit_locations_base\n",
    "                )\n",
    "            },\n",
    "            subspaces=[[[0]] * batch_size, [[1]] * batch_size],\n",
    "        )\n",
    "    elif (\n",
    "        batch[\"intervention_id\"][0] == 0\n",
    "    ):  # Intervention on just the high-level variable 'WX'\n",
    "        _, counterfactual_outputs = handcrafted(\n",
    "            {\"inputs_embeds\": batch[\"input_ids\"]},  # base\n",
    "            [{\"inputs_embeds\": batch[\"source_input_ids\"][:, 0]}, None],  # sources\n",
    "            {\"sources->base\": (\n",
    "                [[[0]] * batch_size, None],  # unit_locations_sources\n",
    "                [[[0]] * batch_size, None]  # unit_locations_base\n",
    "            )},  \n",
    "            subspaces=[[[0]] * batch_size, None],\n",
    "        )\n",
    "    elif (\n",
    "        batch[\"intervention_id\"][0] == 1\n",
    "    ):  # Intervention on just the high-level variable 'YZ'\n",
    "        _, counterfactual_outputs = handcrafted(\n",
    "            {\"inputs_embeds\": batch[\"input_ids\"]},\n",
    "            [None, {\"inputs_embeds\": batch[\"source_input_ids\"][:, 0]}],\n",
    "            {\"sources->base\": ([None, [[0]] * batch_size], [None, [[0]] * batch_size])},\n",
    "            subspaces=[None, [[1]] * batch_size],\n",
    "        )\n",
    "    preds.append(counterfactual_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handcrafted.activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.cat(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we can see that our handcrafted neural network is a perfect implementation of the high-level algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       997\n",
      "         1.0       1.00      1.00      1.00      1051\n",
      "\n",
      "    accuracy                           1.00      2048\n",
      "   macro avg       1.00      1.00      1.00      2048\n",
      "weighted avg       1.00      1.00      1.00      2048\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        torch.tensor([x[\"labels\"] for x in dataset]).cpu(), preds.argmax(1).cpu()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an MLP to Solve Hierarchical Equality\n",
    "\n",
    "We've now seen how to perform causal abstraction analysis on a simple handcrafted neural networks. We turn now to training a neural network to perform the hierarchical equality task with a 4 dimensional vector embedding for each object. We define an input sampler to provide an infinite stream of new entities, rather than relying on a fixed set of vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 4\n",
    "\n",
    "\n",
    "def input_sampler():\n",
    "    A = randvec(4)\n",
    "    B = randvec(4)\n",
    "    C = randvec(4)\n",
    "    D = randvec(4)\n",
    "    x = random.randint(1, 4)\n",
    "    if x == 1:\n",
    "        return {\"W\": A, \"X\": B, \"Y\": C, \"Z\": D}\n",
    "    elif x == 2:\n",
    "        return {\"W\": A, \"X\": A, \"Y\": B, \"Z\": B}\n",
    "    elif x == 3:\n",
    "        return {\"W\": A, \"X\": A, \"Y\": C, \"Z\": D}\n",
    "    elif x == 4:\n",
    "        return {\"W\": A, \"X\": B, \"Y\": C, \"Z\": C}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 1048576\n",
    "batch_size = 1024\n",
    "\n",
    "examples = equality_model.generate_factual_dataset(n_examples, input_sampler)\n",
    "\n",
    "X = torch.stack([example['input_ids'] for example in examples])\n",
    "y = torch.stack([example['labels'] for example in examples])\n",
    "\n",
    "# X = X.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples in this dataset are 8-dimensional vectors: the concatenation of 4 2-dimensional vectors. Here's the first example with its label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.7200,  0.6300,  1.0000,  0.6900, -0.7200,  0.6300,  1.0000,  0.6900,\n",
       "          0.0800, -0.8800, -0.0400, -0.0400, -0.5200, -0.8500, -0.6400,  0.6400]),\n",
       " tensor([0.]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0], y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label for this example is determined by whether the equality value for the first two inputs matches the equality value for the second two inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left = torch.equal(X[0][:embedding_dim], X[0][embedding_dim : embedding_dim * 2])\n",
    "\n",
    "left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right = torch.equal(\n",
    "    X[0][embedding_dim * 2 : embedding_dim * 3], X[0][embedding_dim * 3 :]\n",
    ")\n",
    "\n",
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(left == right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a three layer neural network with a ReLU activation function this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPForClassification(\n",
       "  (mlp): MLPModel(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-2): 3 x MLPBlock(\n",
       "        (ff1): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (act): ReLU()\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (score): Linear(in_features=16, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = MLPConfig(\n",
    "    h_dim=embedding_dim * 4,\n",
    "    activation_function=\"relu\",\n",
    "    n_layer=3,\n",
    "    num_classes=2,\n",
    "    pdrop=0.0,\n",
    ")\n",
    "config, tokenizer, trained = create_mlp_classifier(config)\n",
    "trained.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16])"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained.mlp.h[1].ff1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_dict(\n",
    "    {\n",
    "        \"labels\": [\n",
    "            torch.FloatTensor([0, 1]) if i == 1 else torch.FloatTensor([1, 0])\n",
    "            for i in y\n",
    "        ],\n",
    "        \"inputs_embeds\": X,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=0.001,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=trained,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=train_ds,\n",
    "    compute_metrics=lambda x: {\n",
    "        \"accuracy\": classification_report(\n",
    "            x[0].argmax(1), x[1].argmax(1), output_dict=True\n",
    "        )[\"accuracy\"]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This neural network achieves perfect performance on its train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267ad9a4be754be89e338aa6240214e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3072 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "Checkpoint destination directory test_trainer\\checkpoint-500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6401, 'grad_norm': 0.23116283118724823, 'learning_rate': 0.0008372395833333334, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory test_trainer\\checkpoint-1000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.195, 'grad_norm': 0.18332688510417938, 'learning_rate': 0.0006744791666666667, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6718015efcb34628b758ff359877ff36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07157168537378311, 'eval_accuracy': 0.9790763854980469, 'eval_runtime': 37.061, 'eval_samples_per_second': 28293.236, 'eval_steps_per_second': 27.63, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory test_trainer\\checkpoint-1500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0472, 'grad_norm': 0.15438705682754517, 'learning_rate': 0.00051171875, 'epoch': 1.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory test_trainer\\checkpoint-2000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.025, 'grad_norm': 0.1841142624616623, 'learning_rate': 0.00034895833333333334, 'epoch': 1.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade15e1911a840ceb47b8c372e241568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.019501574337482452, 'eval_accuracy': 0.9946937561035156, 'eval_runtime': 37.7091, 'eval_samples_per_second': 27806.989, 'eval_steps_per_second': 27.155, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory test_trainer\\checkpoint-2500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0177, 'grad_norm': 0.18471458554267883, 'learning_rate': 0.00018619791666666665, 'epoch': 2.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory test_trainer\\checkpoint-3000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0149, 'grad_norm': 0.15878142416477203, 'learning_rate': 2.34375e-05, 'epoch': 2.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6611af998ca848efbcb7ad9afb3bc344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.014224954880774021, 'eval_accuracy': 0.9962148666381836, 'eval_runtime': 42.0755, 'eval_samples_per_second': 24921.316, 'eval_steps_per_second': 24.337, 'epoch': 3.0}\n",
      "{'train_runtime': 257.663, 'train_samples_per_second': 12208.692, 'train_steps_per_second': 11.923, 'train_loss': 0.15329135081265122, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "_ = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a separate causal model with vector representations distinct from those used in training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi!\n"
     ]
    }
   ],
   "source": [
    "variables = [\"W\", \"X\", \"Y\", \"Z\", \"WX\", \"YZ\", \"O\"]\n",
    "\n",
    "number_of_test_entities = 100\n",
    "\n",
    "reps = [randvec(embedding_dim) for _ in range(number_of_test_entities)]\n",
    "values = {variable: reps for variable in [\"W\", \"X\", \"Y\", \"Z\"]}\n",
    "values[\"WX\"] = [True, False]\n",
    "values[\"YZ\"] = [True, False]\n",
    "values[\"O\"] = [True, False]\n",
    "\n",
    "parents = {\n",
    "    \"W\": [],\n",
    "    \"X\": [],\n",
    "    \"Y\": [],\n",
    "    \"Z\": [],\n",
    "    \"WX\": [\"W\", \"X\"],\n",
    "    \"YZ\": [\"Y\", \"Z\"],\n",
    "    \"O\": [\"WX\", \"YZ\"],\n",
    "}\n",
    "\n",
    "\n",
    "def FILLER():\n",
    "    return reps[0]\n",
    "\n",
    "\n",
    "functions = {\n",
    "    \"W\": FILLER,\n",
    "    \"X\": FILLER,\n",
    "    \"Y\": FILLER,\n",
    "    \"Z\": FILLER,\n",
    "    \"WX\": lambda x, y: np.array_equal(x, y),\n",
    "    \"YZ\": lambda x, y: np.array_equal(x, y),\n",
    "    \"O\": lambda x, y: x == y,\n",
    "}\n",
    "\n",
    "pos = {\n",
    "    \"W\": (0, 0),\n",
    "    \"X\": (1, 0.1),\n",
    "    \"Y\": (2, 0.2),\n",
    "    \"Z\": (3, 0),\n",
    "    \"WX\": (1, 2),\n",
    "    \"YZ\": (2, 2),\n",
    "    \"O\": (1.5, 3),\n",
    "}\n",
    "\n",
    "\n",
    "test_equality_model = CausalModel(variables, values, parents, functions, pos=pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our trained model generalizes perfectly this test set consisting of distinct vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171e7809005a4020ab22e408905323ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      1.00      5026\n",
      "         1.0       1.00      0.99      1.00      4974\n",
      "\n",
      "    accuracy                           1.00     10000\n",
      "   macro avg       1.00      1.00      1.00     10000\n",
      "weighted avg       1.00      1.00      1.00     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = test_equality_model.generate_factual_dataset(10000, input_sampler)\n",
    "print(\"Test Results\")\n",
    "\n",
    "test_ds = Dataset.from_dict(\n",
    "    {\n",
    "        \"labels\": [\n",
    "            torch.FloatTensor([0, 1]) if example['labels'].item() == 1 else torch.FloatTensor([1, 0])\n",
    "            for example in examples\n",
    "        ],\n",
    "        \"inputs_embeds\": torch.stack([example['input_ids'] for example in examples]),\n",
    "    }\n",
    ")\n",
    "\n",
    "test_preds = trainer.predict(test_ds)\n",
    "y_test = [example['labels'].item() for example in examples]\n",
    "\n",
    "print(classification_report(y_test, test_preds[0].argmax(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it implement our high-level model of the problem, though?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Alignment Search\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously handcrafted the weights of a network so the two high-level variables are perfectly stored in two non-overlapping sets of neurons in the first layer of the network. However, the trained network won't have axis aligned representations of high-level concepts. Rather, the two high-level variables will be encoded in multidimensional linear subspaces of the first layer in the network.  \n",
    "\n",
    "To learn these subspaces, we define an IntervenableConfig that allows us to target the first layer of in the network after it has been rotated by an orthogonal matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = IntervenableConfig(\n",
    "    model_type=type(trained),\n",
    "    representations=[\n",
    "        RepresentationConfig(\n",
    "            0,  # layer\n",
    "            \"block_output\",  # intervention type\n",
    "            \"pos\",  # intervention unit is now aligne with tokens\n",
    "            1,  # max number of unit\n",
    "            subspace_partition=None,  # binary partition with equal sizes\n",
    "            intervention_link_key=0,\n",
    "        ),\n",
    "        RepresentationConfig(\n",
    "            0,  # layer\n",
    "            \"block_output\",  # intervention type\n",
    "            \"pos\",  # intervention unit is now aligne with tokens\n",
    "            1,  # max number of unit\n",
    "            subspace_partition=None,  # binary partition with equal sizes,\n",
    "            intervention_link_key=0,\n",
    "        ),\n",
    "    ],\n",
    "    intervention_types=RotatedSpaceIntervention,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/by/rktr_w596p97pmt8_cbknvs80000gn/T/ipykernel_45437/4050597916.py:62: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  logging.warn(\n",
      "WARNING:root:Detected use_fast=True means the intervention location will be static within a batch.\n",
      "\n",
      "In case multiple location tags are passed only the first one will be considered\n"
     ]
    }
   ],
   "source": [
    "intervenable = IntervenableModel(config, trained, use_fast=True)\n",
    "#intervenable.set_device(\"cuda\")\n",
    "intervenable.disable_model_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer.0.comp.block_output.unit.pos.nunit.1#0': (RotatedSpaceIntervention(\n",
       "    (rotate_layer): ParametrizedRotateLayer(\n",
       "      (parametrizations): ModuleDict(\n",
       "        (weight): ParametrizationList(\n",
       "          (0): _Orthogonal()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       "  <bound method Module.register_forward_hook of MLPBlock(\n",
       "    (ff1): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (act): ReLU()\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )>),\n",
       " 'layer.0.comp.block_output.unit.pos.nunit.1#1': (RotatedSpaceIntervention(\n",
       "    (rotate_layer): ParametrizedRotateLayer(\n",
       "      (parametrizations): ModuleDict(\n",
       "        (weight): ParametrizationList(\n",
       "          (0): _Orthogonal()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       "  <bound method Module.register_forward_hook of MLPBlock(\n",
       "    (ff1): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (act): ReLU()\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )>)}"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intervenable.interventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "gradient_accumulation_steps = 1\n",
    "total_step = 0\n",
    "target_total_step = len(dataset) * epochs\n",
    "\n",
    "t_total = int(len(dataset) * epochs)\n",
    "optimizer_params = []\n",
    "for k, v in intervenable.interventions.items():\n",
    "    optimizer_params += [{\"params\": v[0].rotate_layer.parameters()}]\n",
    "    break\n",
    "optimizer = torch.optim.Adam(optimizer_params, lr=0.001)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds, eval_labels):\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "    for eval_pred, eval_label in zip(eval_preds, eval_labels):\n",
    "        total_count += 1\n",
    "        correct_count += eval_pred == eval_label\n",
    "    accuracy = float(correct_count) / float(total_count)\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "def compute_loss(outputs, labels):\n",
    "    CE = torch.nn.CrossEntropyLoss()\n",
    "    return CE(outputs, labels)\n",
    "\n",
    "\n",
    "def batched_random_sampler(data):\n",
    "    batch_indices = [_ for _ in range(int(len(data) / batch_size))]\n",
    "    random.shuffle(batch_indices)\n",
    "    for b_i in batch_indices:\n",
    "        for i in range(b_i * batch_size, (b_i + 1) * batch_size):\n",
    "            yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_sampler(*args, **kwargs):\n",
    "    A = randvec(4)\n",
    "    B = randvec(4)\n",
    "    C = randvec(4)\n",
    "    D = randvec(4)\n",
    "    if kwargs.get('output_var', None) is None:\n",
    "        return random.choice([\n",
    "            {\"W\": A, \"X\": B, \"Y\": C, \"Z\": D},\n",
    "            {\"W\": A, \"X\": A, \"Y\": B, \"Z\": B},\n",
    "            {\"W\": A, \"X\": A, \"Y\": C, \"Z\": D},\n",
    "            {\"W\": A, \"X\": B, \"Y\": C, \"Z\": C}\n",
    "        ])\n",
    "    elif kwargs['output_var'] == 'WX' and kwargs['output_var_value']:\n",
    "        return random.choice([\n",
    "            {\"W\": A, \"X\": A, \"Y\": C, \"Z\": D},\n",
    "            {\"W\": A, \"X\": A, \"Y\": C, \"Z\": C}\n",
    "        ])\n",
    "    elif kwargs['output_var'] == 'WX' and not kwargs['output_var_value']:\n",
    "        return random.choice([\n",
    "            {\"W\": A, \"X\": B, \"Y\": C, \"Z\": D},\n",
    "            {\"W\": A, \"X\": B, \"Y\": C, \"Z\": C}\n",
    "        ])\n",
    "    elif kwargs['output_var'] == 'YZ' and kwargs['output_var_value']:\n",
    "        return random.choice([\n",
    "            {\"W\": A, \"X\": B, \"Y\": C, \"Z\": C},\n",
    "            {\"W\": A, \"X\": A, \"Y\": C, \"Z\": C}\n",
    "        ])\n",
    "    else:\n",
    "        return random.choice([\n",
    "            {\"W\": A, \"X\": B, \"Y\": C, \"Z\": D},\n",
    "            {\"W\": A, \"X\": A, \"Y\": C, \"Z\": D}\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again generate a counterfactual dataset using our high-level causal model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 1280000\n",
    "batch_size = 6400\n",
    "train_dataset = equality_model.generate_counterfactual_dataset(\n",
    "    n_examples, intervention_id, batch_size, sampler=input_sampler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train the orthgonal matrix to be such that the first four dimensions in the rotated space encode the high-level variable 'WX' and the second four dimensions encode the high-level variable 'YZ'. \n",
    "\n",
    "Again, we check the intervention_id for each batch of training data in order to determine whether to intervene of the first four rotated dimensions ('WX' is targetted at the high-level), the last four rotated dimensions ('YZ' is targetted at the high-level), or all of the dimensions ('WX' and 'YZ' are both targetted at the high-level). \n",
    "\n",
    "We can train the rotation matrix such that we get perfect interchange intervention accuracy, meaning the trained network perfectly implements the high-level algorithm on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 4\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0, 1, 2, 3, 4, 5, 6, 7],\n",
       "  [0, 1, 2, 3, 4, 5, 6, 7],\n",
       "  [0, 1, 2, 3, 4, 5, 6, 7],\n",
       "  [0, 1, 2, 3, 4, 5, 6, 7],\n",
       "  [0, 1, 2, 3, 4, 5, 6, 7],\n",
       "  [0, 1, 2, 3, 4, 5, 6, 7],\n",
       "  [0, 1, 2, 3, 4, 5, 6, 7],\n",
       "  [0, 1, 2, 3, 4, 5, 6, 7],\n",
       "  [0, 1, 2, 3, 4, 5, 6, 7],\n",
       "  [0, 1, 2, 3, 4, 5, 6, 7],\n",
       "  [0, 1, 2, 3, 4, 5, 6, 7],\n",
       "  [0, 1, 2, 3, 4, 5, 6, 7],\n",
       "  [0, 1, 2, 3, 4, 5, 6, 7],\n",
       "  [0, 1, 2, 3, 4, 5, 6, 7],\n",
       "  [0, 1, 2, 3, 4, 5, 6, 7],\n",
       "  [0, 1, 2, 3, 4, 5, 6, 7]],)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[_ for _ in range(0, embedding_dim * 2)]] * batch_size,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[_ for _ in range(0, embedding_dim * 2)]] * batch_size\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 9, 10, 11, 12, 13, 14, 15],\n",
       " [8, 9, 10, 11, 12, 13, 14, 15],\n",
       " [8, 9, 10, 11, 12, 13, 14, 15],\n",
       " [8, 9, 10, 11, 12, 13, 14, 15],\n",
       " [8, 9, 10, 11, 12, 13, 14, 15],\n",
       " [8, 9, 10, 11, 12, 13, 14, 15],\n",
       " [8, 9, 10, 11, 12, 13, 14, 15],\n",
       " [8, 9, 10, 11, 12, 13, 14, 15],\n",
       " [8, 9, 10, 11, 12, 13, 14, 15],\n",
       " [8, 9, 10, 11, 12, 13, 14, 15],\n",
       " [8, 9, 10, 11, 12, 13, 14, 15],\n",
       " [8, 9, 10, 11, 12, 13, 14, 15],\n",
       " [8, 9, 10, 11, 12, 13, 14, 15],\n",
       " [8, 9, 10, 11, 12, 13, 14, 15],\n",
       " [8, 9, 10, 11, 12, 13, 14, 15],\n",
       " [8, 9, 10, 11, 12, 13, 14, 15]]"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[_ for _ in range(embedding_dim * 2, embedding_dim * 4)]] * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intervention trainable parameters:  256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0: 0it [00:00, ?it/s] [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0: 200it [01:17,  2.59it/s, loss=tensor(0.4580, device='cuda:0', grad_fn=<NllLossBackward0>), acc=0.872]\n",
      "Epoch: 1: 200it [00:54,  3.69it/s, loss=tensor(0.4646, device='cuda:0', grad_fn=<NllLossBackward0>), acc=0.89] \n",
      "Epoch: 2: 200it [00:53,  3.71it/s, loss=tensor(0.1925, device='cuda:0', grad_fn=<NllLossBackward0>), acc=0.962]\n",
      "Epoch: 3: 200it [00:53,  3.71it/s, loss=tensor(0.5047, device='cuda:0', grad_fn=<NllLossBackward0>), acc=0.837]\n",
      "Epoch: 4: 200it [00:54,  3.70it/s, loss=tensor(0.1448, device='cuda:0', grad_fn=<NllLossBackward0>), acc=0.969]\n",
      "Epoch: 5: 200it [00:52,  3.81it/s, loss=tensor(0.1444, device='cuda:0', grad_fn=<NllLossBackward0>), acc=0.967]\n",
      "Epoch: 6: 200it [00:58,  3.40it/s, loss=tensor(0.1562, device='cuda:0', grad_fn=<NllLossBackward0>), acc=0.97] \n",
      "Epoch: 7: 200it [01:04,  3.09it/s, loss=tensor(0.1703, device='cuda:0', grad_fn=<NllLossBackward0>), acc=0.958]\n",
      "Epoch: 8: 200it [01:05,  3.07it/s, loss=tensor(0.1553, device='cuda:0', grad_fn=<NllLossBackward0>), acc=0.959]\n",
      "Epoch: 9: 200it [01:04,  3.10it/s, loss=tensor(0.1505, device='cuda:0', grad_fn=<NllLossBackward0>), acc=0.967]\n",
      "Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [09:59<00:00, 59.91s/it]\n"
     ]
    }
   ],
   "source": [
    "intervenable.model.train()  # train enables drop-off but no grads\n",
    "print(\"intervention trainable parameters: \", intervenable.count_parameters())\n",
    "train_iterator = trange(0, int(epochs), desc=\"Epoch\")\n",
    "\n",
    "for epoch in train_iterator:\n",
    "    epoch_iterator = tqdm(\n",
    "        DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            sampler=batched_random_sampler(train_dataset),\n",
    "        ),\n",
    "        desc=f\"Epoch: {epoch}\",\n",
    "        position=0,\n",
    "        leave=True,\n",
    "    )\n",
    "    for batch in epoch_iterator:\n",
    "        batch[\"input_ids\"] = batch[\"input_ids\"].unsqueeze(1)\n",
    "        batch[\"source_input_ids\"] = batch[\"source_input_ids\"].unsqueeze(2)\n",
    "        batch_size = batch[\"input_ids\"].shape[0]\n",
    "        for k, v in batch.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                batch[k] = v.to(\"cuda\")\n",
    "\n",
    "        if batch[\"intervention_id\"][0] == 2:\n",
    "            _, counterfactual_outputs = intervenable(\n",
    "                {\"inputs_embeds\": batch[\"input_ids\"]},\n",
    "                [\n",
    "                    {\"inputs_embeds\": batch[\"source_input_ids\"][:, 0]},\n",
    "                    {\"inputs_embeds\": batch[\"source_input_ids\"][:, 1]},\n",
    "                ],\n",
    "                {\n",
    "                    \"sources->base\": (\n",
    "                        [[[0]] * batch_size, [[0]] * batch_size],\n",
    "                        [[[0]] * batch_size, [[0]] * batch_size],\n",
    "                    )\n",
    "                },\n",
    "                subspaces=[\n",
    "                    [[_ for _ in range(0, embedding_dim * 2)]] * batch_size,\n",
    "                    [[_ for _ in range(embedding_dim * 2, embedding_dim * 4)]] * batch_size,\n",
    "                ],\n",
    "            )\n",
    "        elif batch[\"intervention_id\"][0] == 0:\n",
    "            _, counterfactual_outputs = intervenable(\n",
    "                {\"inputs_embeds\": batch[\"input_ids\"]},\n",
    "                [{\"inputs_embeds\": batch[\"source_input_ids\"][:, 0]}, None],\n",
    "                {\n",
    "                    \"sources->base\": (\n",
    "                        [[[0]] * batch_size, None],\n",
    "                        [[[0]] * batch_size, None],\n",
    "                    )\n",
    "                },\n",
    "                subspaces=[\n",
    "                    [[_ for _ in range(0, embedding_dim * 2)]] * batch_size,\n",
    "                    None,\n",
    "                ],\n",
    "            )\n",
    "        elif batch[\"intervention_id\"][0] == 1:\n",
    "            _, counterfactual_outputs = intervenable(\n",
    "                {\"inputs_embeds\": batch[\"input_ids\"]},\n",
    "                [None, {\"inputs_embeds\": batch[\"source_input_ids\"][:, 0]}],\n",
    "                {\n",
    "                    \"sources->base\": (\n",
    "                        [None, [[0]] * batch_size],\n",
    "                        [None, [[0]] * batch_size],\n",
    "                    )\n",
    "                },\n",
    "                subspaces=[\n",
    "                    None,\n",
    "                    [[_ for _ in range(embedding_dim * 2, embedding_dim * 4)]] * batch_size,\n",
    "                ],\n",
    "            )\n",
    "        eval_metrics = compute_metrics(\n",
    "            counterfactual_outputs[0].argmax(1), batch[\"labels\"].squeeze()\n",
    "        )\n",
    "\n",
    "        # loss and backprop\n",
    "        loss = compute_loss(\n",
    "            counterfactual_outputs[0], batch[\"labels\"].squeeze().to(torch.long)\n",
    "        )\n",
    "\n",
    "        epoch_iterator.set_postfix({\"loss\": loss, \"acc\": eval_metrics[\"accuracy\"]})\n",
    "\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        if total_step % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            intervenable.set_zero_grad()\n",
    "        total_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's more, is it generalizes unseen test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_equality_model.generate_counterfactual_dataset(\n",
    "    10000, intervention_id, batch_size, device=\"cuda:0\", sampler=input_sampler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.96      0.97      6407\n",
      "         1.0       0.96      0.98      0.97      6393\n",
      "\n",
      "    accuracy                           0.97     12800\n",
      "   macro avg       0.97      0.97      0.97     12800\n",
      "weighted avg       0.97      0.97      0.97     12800\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_labels = []\n",
    "eval_preds = []\n",
    "with torch.no_grad():\n",
    "    epoch_iterator = tqdm(DataLoader(test_dataset, batch_size), desc=f\"Test\")\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        for k, v in batch.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                batch[k] = v.to(\"cuda\")\n",
    "        batch[\"input_ids\"] = batch[\"input_ids\"].unsqueeze(1)\n",
    "        batch[\"source_input_ids\"] = batch[\"source_input_ids\"].unsqueeze(2)\n",
    "        if batch[\"intervention_id\"][0] == 2:\n",
    "            _, counterfactual_outputs = intervenable(\n",
    "                {\"inputs_embeds\": batch[\"input_ids\"]},\n",
    "                [\n",
    "                    {\"inputs_embeds\": batch[\"source_input_ids\"][:, 0]},\n",
    "                    {\"inputs_embeds\": batch[\"source_input_ids\"][:, 1]},\n",
    "                ],\n",
    "                {\n",
    "                    \"sources->base\": (\n",
    "                        [[[0]] * batch_size, [[0]] * batch_size],\n",
    "                        [[[0]] * batch_size, [[0]] * batch_size],\n",
    "                    )\n",
    "                },\n",
    "                subspaces=[\n",
    "                    [[_ for _ in range(0, embedding_dim * 2)]] * batch_size,\n",
    "                    [[_ for _ in range(embedding_dim * 2, embedding_dim * 4)]]\n",
    "                    * batch_size,\n",
    "                ],\n",
    "            )\n",
    "        elif batch[\"intervention_id\"][0] == 0:\n",
    "            _, counterfactual_outputs = intervenable(\n",
    "                {\"inputs_embeds\": batch[\"input_ids\"]},\n",
    "                [{\"inputs_embeds\": batch[\"source_input_ids\"][:, 0]}, None],\n",
    "                {\n",
    "                    \"sources->base\": (\n",
    "                        [[[0]] * batch_size, None],\n",
    "                        [[[0]] * batch_size, None],\n",
    "                    )\n",
    "                },\n",
    "                subspaces=[\n",
    "                    [[_ for _ in range(0, embedding_dim * 2)]] * batch_size,\n",
    "                    None,\n",
    "                ],\n",
    "            )\n",
    "        elif batch[\"intervention_id\"][0] == 1:\n",
    "            _, counterfactual_outputs = intervenable(\n",
    "                {\"inputs_embeds\": batch[\"input_ids\"]},\n",
    "                [None, {\"inputs_embeds\": batch[\"source_input_ids\"][:, 0]}],\n",
    "                {\n",
    "                    \"sources->base\": (\n",
    "                        [None, [[0]] * batch_size],\n",
    "                        [None, [[0]] * batch_size],\n",
    "                    )\n",
    "                },\n",
    "                subspaces=[\n",
    "                    None,\n",
    "                    [[_ for _ in range(embedding_dim * 2, embedding_dim * 4)]]\n",
    "                    * batch_size,\n",
    "                ],\n",
    "            )\n",
    "        eval_labels += [batch[\"labels\"]]\n",
    "        eval_preds += [torch.argmax(counterfactual_outputs[0], dim=1)]\n",
    "print(classification_report(torch.cat(eval_labels).cpu(), torch.cat(eval_preds).cpu()))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "933b0a94e0d88ac80a17cb26ca3d8d36930c12815b02a2885c1925c2b1ae3c33"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
